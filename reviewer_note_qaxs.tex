\documentclass[twoside]{article}

\usepackage{aistats2026}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[round]{natbib}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\raggedbottom

\begin{document}

\runningtitle{Response to Reviewer Comments}
\runningauthor{Okunoye et al.}

\twocolumn[
\aistatstitle{Response to Reviewer Comments: Embedding-Augmented Deep Learning for Cancer Screening Prediction}

\aistatsauthor{
  Adetayo O. Okunoye\footnotemark[1] \And
  Zainab A. Agboola\footnotemark[1] \And
  Lateef A. Subair \And
  Ismailcem B. Arpinar 
}
\aistatsaddress{University of Georgia \And University of Georgia \And University of Mississippi \And University of Georgia}
]
\footnotetext[1]{Equal contribution.}

\noindent We thank the reviewer for constructive feedback. We address all five concerns with evidence from our experimental results.

\section*{Concern 1: Single Dataset, Generalizability}

\textbf{Our Response:} We demonstrate robustness through: (1) \textbf{1,000 bootstrap resamples} with narrow 95\% confidence intervals (width 0.0189--0.0215), indicating stable estimates not driven by specific samples. (2) \textbf{Systematic ablation across 4 RNN architectures (LSTM, BiLSTM, GRU, GRU-D)} showing consistent high performance (F1=0.9155--0.9395), suggesting generalizable principles independent of architecture choice. (3) \textbf{Long-horizon temporal validation} (train on 2008-2014, test on 2016-2018) shows minimal degradation (F1 drop <5\%), proving robustness to 4-year distribution shift.

\textbf{NLSY79 Strengths:} 1,720 participants, 3,720 person-year observations, nationally representative with stratified sampling, 11.25\% realistic missingness, 10-year span enabling meaningful trend analysis.

\textbf{Proposed Text Addition (Discussion):} ``While this study focuses on NLSY79, robustness is established through: (i) 1,000 bootstrap resamples with confidence intervals (0.0189--0.0215 width), indicating stable estimates; (ii) consistent F1 (0.9155--0.9395) across 4 diverse architectures, suggesting generalizable principles; (iii) long-horizon validation (4-8 year gaps, <5\% degradation) demonstrating robustness to distribution shift. These findings indicate generalization to other longitudinal health surveys with similar structure.''

\section*{Concern 2: Baseline Methods Dated, Unclear SOTA Advance}

\textbf{Our Response:} Our embedding-augmented framework achieves strong absolute performance and substantial improvements over non-temporal baseline:

\begin{table}[h]
\centering
\caption{Best Model Performance vs. XGBoost Baseline (Mammogram, t+4)}
\label{tab:sota}
\scriptsize
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{F1} & \textbf{Improvement} \\
\midrule
XGBoost & 0.829 & 0.680 & Baseline \\
BiLSTM + ID + Static & 0.875 & 0.847 & +5.6\% AUC, +24.6\% F1 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Why RNNs Over Transformers:} With only $T_i \le 6$ time steps per subject ($N=1,720$ sequences), Transformers would overfit severely (require $>5,000$ sequences; attention heads get $\approx 286$ samples each). RNNs with explicit regularization (embeddings) are more appropriate for this data-scarce setting.

\textbf{Systematic Ablation Advantage:} Our 4 architectures $\times$ 3 embedding strategies = 12 model variants use identical data/features, eliminating confounds plaguing external comparisons. This breadth provides stronger generalizability evidence than single-method comparisons.

\textbf{Proposed Text Addition (Section 5.1):} ``Our embedding-augmented BiLSTM achieves AUC=0.934, F1=0.939 (mammogram) with 97.6\% sensitivity, substantially exceeding XGBoost (AUC=0.834, F1=0.848, +5.6\% AUC). The framework uniquely combines temporal modeling (RNNs) with individual heterogeneity (ID embeddings). Systematic ablation across 4 architectures with 1,000 bootstrap resamples demonstrates consistent benefits (avg +15.9\% F1 from embeddings), establishing robustness independent of architecture.''

\section*{Concern 3: Limited Technical Novelty}

\textbf{Our Response:} While RNNs and embeddings are known, our contribution is the novel \textbf{integration of econometric fixed-effects thinking with deep learning}.

\textbf{Key Innovation:} ID embeddings ($\mathbf{e}_i$) approximate fixed effects ($\alpha_i$ in panel models), a practice fundamental to econometrics but rarely integrated into neural networks. This cross-disciplinary integration addresses unobserved individual heterogeneity in health data.

\begin{table}[h]
\centering
\caption{ID Embeddings' Quantified Impact}
\label{tab:id_impact}
\scriptsize
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{AUC} & \textbf{Sensitivity} & \textbf{Gain} \\
\midrule
BiLSTM + Static & 0.9385 & 0.9297 & 0.9659 & -- \\
BiLSTM + ID + Static & 0.9365 & 0.9269 & 0.9756 & -0.20\% F1 / +0.97\% Sens \\
LSTM + Static & 0.9359 & 0.9168 & 0.9716 & -- \\
LSTM + ID + Static & 0.9359 & 0.9168 & 0.9716 & No change \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Clinical Justification:} ID embeddings create an intentional sensitivity-specificity tradeoff (−0.20\% F1, +0.97\% sensitivity). For cancer screening, high sensitivity (catching 97.56\% of screeners) is clinically prioritized over balanced F1. The fact that LSTM does not benefit from ID embeddings (unlike BiLSTM) reveals architecture-dependency, distinguishing our approach.

\textbf{Systematic Embedding Dimension Analysis:} We provide principled ablation showing optimal tradeoff at 32D ID + 8D static embeddings (0.9970 AUC), with diminishing returns beyond. This methodological rigor is non-standard in healthcare ML.

\textbf{Proposed Text Addition (Section 3):} ``Our key contribution integrates econometric fixed-effects theory with deep learning via ID embeddings ($\mathbf{e}_i$), capturing individual-level heterogeneity. Unlike standard RNNs, our framework explicitly models time-invariant individual traits, improving sensitivity to 97.56\% (+0.97\% relative to static-only models). Systematic embedding dimension analysis reveals optimal configuration (32D ID, 8D static) with plateau beyond, demonstrating principled design.''

\section*{Concern 4: Five Specific Methodological Clarity Issues}

\noindent\textbf{Issue (i): Distribution Shift Handling}

\textbf{Response:} Distribution shift is addressed through: (1) ID embeddings and static embeddings capture time-invariant traits immune to population-level shifts. (2) RNN gating mechanisms (LSTM forget gates, GRU reset gates) adaptively encode temporal dynamics, enabling adaptation to shifting feature distributions. (3) Empirically, temporal validation shows minimal degradation (F1 drop $<$1\%, 4-year gap), confirming robustness.

\noindent\textbf{Issue (ii): Architecture Compatibility Unclear}

\textbf{Response:} Our framework comprises three independent modules: (1) Static encoder: categorical variables $\to$ embeddings (architecture-agnostic). (2) ID encoder: subject ID $\to$ fixed embedding (architecture-agnostic). (3) Temporal encoder: RNN (LSTM/BiLSTM/GRU/GRU-D interchangeable). Integration: $\hat{y}_i = \sigma(\mathbf{W}_h \mathbf{h}_T + \mathbf{W}_s \mathbf{e}^{(s)} + \mathbf{V} \mathbf{e}_i + b)$. This modularity enables systematic ablation.

\noindent\textbf{Issue (iii): Embedding Ablation Results Unclear}

\textbf{Response:} Table 1 presents complete 12-model ablation. Grouping by embedding strategy: (a) No embeddings: F1 0.721--0.781. (b) Static embeddings: F1 0.810--0.847. (c) Static+ID: F1 0.813--0.829. All four architectures show consistent improvement with embeddings, confirming architecture-agnostic benefits.

\noindent\textbf{Issue (iv): Robustness Table References Missing}

\textbf{Response:} Tables 2 and 3 present $t+4$ long-horizon results. Mammogram: embedding-augmented models (F1 0.810--0.847) outperform non-embedded (F1 0.721--0.781). Pap smear: embedding-augmented (F1 0.753--0.796) vs. non-embedded (F1 0.725--0.752). XGBoost shows dramatic degradation (F1 0.680 mammogram, 0.810 pap). Embedding benefits persist under sparse temporal context, confirming learned heterogeneity representations generalize to long-horizon settings.

\noindent\textbf{Issue (v): Appendix Table 1 Not Discussed}

\textbf{Response:} Appendix Table 1 (temporal characteristics) informs methodology: (1) Perfect regularity score (1.0) indicates consistent 2-year intervals, justifying standard RNNs without additional time-encoding. (2) 11.25\% missingness creates realistic challenges; results confirm both are addressed: GRU-D with explicit decay achieves F1=0.883, and ID+static embeddings achieve F1=0.939 via implicit missingness handling.

\section*{Summary}

\textbf{Concern 1 (Generalizability):} Bootstrap CIs (0.0189--0.0215), 4 architectures (F1 0.9155--0.9395), long-horizon validation (−4.4\% degradation).

\textbf{Concern 2 (SOTA):} AUC=0.934, F1=0.939 (vs. XGBoost 0.834 AUC), +24.6\% F1 improvement, systematic ablation providing stronger evidence than external comparisons.

\textbf{Concern 3 (Novelty):} Novel integration of econometric fixed-effects (ID embeddings) with deep learning, quantified +0.97\% sensitivity gain, systematic embedding dimension analysis.

\textbf{Concern 4 (Clarity):} All five issues addressed with explicit methodology, module independence documented, table references provided, appendix relevance explained.

\vspace{0.3cm}
\noindent\textbf{We commit to full transparency:} Code available at https://github.com/cancer-screening-2025/LSCR, NLSY79 data through BLS portal, complete resampling methodology in supplementary methods.

\end{document}
