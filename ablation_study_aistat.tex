\documentclass[twoside]{article}

\usepackage{aistats2026} % for anonymous preprint
%\usepackage[preprint]{aistats2026} % change to [accepted] after acceptance


\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
%\usepackage{natbib}
\usepackage[round]{natbib}
\usepackage{tikz}

\usetikzlibrary{positioning, arrows.meta, shapes, calc}
%\usepackage{float}
% ---------- in your preamble ----------
\usepackage{placeins} % for \FloatBarrier
% DO NOT load \usepackage{float}; avoid [H]
\usepackage{hyperref}

\usepackage[capitalize,noabbrev]{cleveref}
% Prevent LaTeX from stretching columns/pages vertically (reduces big gaps)
\raggedbottom

\begin{document}

\runningtitle{Forecasting Cancer Screening with Embedding-Augmented DL}
\runningauthor{Okunoye, Agboola, Subair, Arpinar}

\twocolumn[
\aistatstitle{Forecasting Cancer Screening Behavior from Longitudinal Data with Embedding-Augmented Deep Learning}

% Row 1
\aistatsauthor{
  Adetayo O. Okunoye\footnotemark[1] \And
  Zainab A. Agboola\footnotemark[1] \And
  Lateef A. Subair \And
  Ismailcem B. Arpinar 
}
\aistatsaddress{
  University of Georgia \And
  University of Georgia \And
  University of Mississippi \And
  University of Georgia
}
]
\footnotetext[1]{Equal contribution.}

\begin{abstract}
This paper presents a comprehensive ablation study examining the impact of architectural components on neural network performance for predicting cancer screening behavior. We systematically evaluate the contribution of recurrent neural network variants (GRU, LSTM, BiLSTM), attention mechanisms, temporal encoding strategies (GRU-D, static embeddings, and static ID features), and their interactions. Our experiments on a longitudinal cancer screening dataset ($n=1,720$ observations) demonstrate that architectural choices have substantial effects on model performance, with the BiLSTM + ID + Static configuration achieving optimal F1-score (0.937) and sensitivity (0.976). This study provides evidence-based guidance for practitioners selecting temporal neural network architectures for healthcare prediction tasks.

\textbf{Keywords:} Ablation Study, Temporal Neural Networks, Cancer Screening, Model Selection, Healthcare Prediction
\end{abstract}

\section{Introduction}

Temporal neural networks have become increasingly important for healthcare prediction tasks where longitudinal patient data is available. However, selecting appropriate architectural components remains challenging due to the computational cost of extensive hyperparameter searches and the complexity of interpreting component interactions. This ablation study systematically evaluates architectural choices to quantify their individual and combined effects on predictive performance.

Our main contributions are:
\begin{enumerate}
\item A systematic evaluation of recurrent neural network variants (GRU, LSTM, BiLSTM) with controlled experimental design
\item Quantification of attention mechanism effectiveness in healthcare prediction
\item Assessment of temporal encoding strategies for incomplete and missing data
\item Evidence-based guidance for architecture selection in cancer screening behavior prediction
\end{enumerate}

\section{Experimental Setup}

\subsection{Dataset}
We utilized longitudinal cancer screening data spanning 2008-2018 with $n=1,720$ unique female participants and $n=3,720$ total person-year observations. Target variables included mammography and pap smear screening behaviors. Features included:
\begin{itemize}
\item \textbf{Temporal variables}: Age, time since last screening, screening history
\item \textbf{Demographic features}: Race, education, marital status
\item \textbf{Health indicators}: BMI, health insurance status, self-rated health
\item \textbf{Socioeconomic factors}: Income level, employment status, metropolitan status
\end{itemize}

Data was split into training (70\%), validation (10\%), and test (20\%) sets, with temporal ordering preserved.

\subsection{Architectural Components Evaluated}

We systematically varied three dimensions:

\subsubsection{Recurrent Neural Networks}
\begin{itemize}
\item \textbf{GRU}: 80 hidden units, faster convergence, fewer parameters
\item \textbf{LSTM}: 80 hidden units, enhanced capacity for long-term dependencies
\item \textbf{BiLSTM}: Bidirectional 40 hidden units each direction, captures temporal context in both directions
\end{itemize}

\subsubsection{Temporal Encoding Strategies}
\begin{itemize}
\item \textbf{Static Embeddings}: Learnable embeddings for categorical variables (age groups, health status categories)
\item \textbf{ID + Static}: Individual participant ID embeddings combined with static embeddings to capture patient-specific effects
\item \textbf{GRU-D}: Gated Recurrent Unit with Decay mechanism for explicit handling of temporal irregularities and missing data
\end{itemize}

\subsubsection{Attention Mechanisms}
\begin{itemize}
\item \textbf{No Attention}: Baseline without attention
\item \textbf{With Attention}: Scaled dot-product attention over temporal sequence
\end{itemize}

\subsection{Training Configuration}
\begin{itemize}
\item Optimizer: Adam with learning rate $3 \times 10^{-3}$
\item Loss function: Binary cross-entropy with class weights to address imbalance
\item Batch size: 32
\item Epochs: 100 with early stopping (patience=10 epochs)
\item Dropout: 0.5 on recurrent outputs
\item L2 regularization: $5 \times 10^{-5}$
\end{itemize}

\section{Results and Analysis}

\subsection{Clinical Performance Metrics}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{4pt}
\fontsize{7}{8}\selectfont
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Sens} & \textbf{Spec} & \textbf{PPV} & \textbf{NPV} & \textbf{Acc} \\
\midrule
BiLSTM + ID + Static & \textbf{0.937} & \textbf{0.976} & 0.601 & 0.900 & 0.870 & 0.896 \\
GRU + Attention & 0.939 & 0.963 & 0.679 & 0.917 & 0.831 & 0.902 \\
BiLSTM + Static Embeds & 0.938 & 0.966 & 0.658 & 0.913 & 0.839 & 0.900 \\
LSTM + Static Embeds & 0.936 & 0.972 & 0.613 & 0.903 & 0.854 & 0.895 \\
BiLSTM + Attention & 0.896 & 0.969 & 0.279 & 0.833 & 0.710 & 0.822 \\
GRU-D Basic & 0.915 & 0.954 & 0.520 & 0.880 & 0.752 & 0.861 \\
LSTM + Attention & 0.841 & 0.790 & 0.673 & 0.899 & 0.464 & 0.765 \\
GRU-D + Static Embeds & 0.830 & 0.886 & 0.633 & 0.781 & 0.790 & 0.784 \\
GRU + ID + Static & 0.822 & 0.792 & 0.606 & 0.854 & 0.501 & 0.744 \\
GRU + Static Embeds & 0.833 & 0.787 & 0.619 & 0.884 & 0.440 & 0.751 \\
LSTM + ID + Static & 0.771 & 0.821 & 0.546 & 0.728 & 0.673 & 0.710 \\
GRU-D + ID + Static & 0.771 & 0.821 & 0.546 & 0.728 & 0.673 & 0.710 \\
\bottomrule
\end{tabular}
\caption{Clinical metrics for all models. BiLSTM + ID + Static achieves best sensitivity (97.6\%). GRU + Attention achieves best F1-score (0.939). Sens=Sensitivity, Spec=Specificity, PPV=Positive Predictive Value, NPV=Negative Predictive Value, Acc=Accuracy.}
\label{tab:clinical_metrics}
\end{table*}

\vspace{-0.15cm}

\subsection{Bootstrap Confidence Intervals}

To quantify uncertainty in model performance, we performed 1,000 bootstrap resamples of the test set.

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt}
\fontsize{6.5}{7.5}\selectfont
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{AUC CI} & \textbf{F1} & \textbf{F1 CI} & \textbf{Std(AUC)} & \textbf{Std(F1)} \\
\midrule
GRU + Attention & 0.9313 & [0.915, 0.946] & 0.9395 & [0.930, 0.949] & 0.0077 & 0.0049 \\
BiLSTM + Static & 0.9297 & [0.913, 0.944] & 0.9385 & [0.929, 0.948] & 0.0080 & 0.0049 \\
BiLSTM + ID + Static & 0.9269 & [0.911, 0.942] & 0.9365 & [0.927, 0.946] & 0.0079 & 0.0048 \\
LSTM + Static & 0.9168 & [0.899, 0.933] & 0.9359 & [0.926, 0.945] & 0.0088 & 0.0049 \\
GRU-D Basic & 0.8748 & [0.853, 0.895] & 0.9155 & [0.904, 0.926] & 0.0107 & 0.0056 \\
GRU-D + Static & 0.8483 & [0.829, 0.866] & 0.8301 & [0.813, 0.846] & 0.0093 & 0.0086 \\
BiLSTM + Attention & 0.8297 & [0.807, 0.852] & 0.8957 & [0.884, 0.907] & 0.0116 & 0.0061 \\
\bottomrule
\end{tabular}
\caption{Bootstrap CI (1,000 resamples). Top 7 models shown. Narrow CIs indicate stable performance estimates.}
\label{tab:bootstrap_ci}
\end{table*}

\vspace{-0.15cm}

\subsection{Comprehensive Model Comparison}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3.5pt}
\fontsize{7}{8}\selectfont
\begin{tabular}{@{}lccccccc@{}}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{F1} & \textbf{Sens} & \textbf{Spec} & \textbf{PPV} & \textbf{NPV} \\
\midrule
GRU + Attention & 0.9313 & 0.9395 & 0.9627 & 0.6787 & 0.9172 & 0.8309 \\
BiLSTM + Static & 0.9297 & 0.9385 & 0.9659 & 0.6577 & 0.9126 & 0.8391 \\
BiLSTM + ID + Static & 0.9269 & 0.9365 & 0.9756 & 0.6006 & 0.9004 & 0.8696 \\
LSTM + Static & 0.9168 & 0.9359 & 0.9716 & 0.6126 & 0.9027 & 0.8536 \\
GRU-D Basic & 0.8748 & 0.9155 & 0.9537 & 0.5195 & 0.8801 & 0.7522 \\
GRU-D + Static & 0.8483 & 0.8301 & 0.8860 & 0.6326 & 0.7809 & 0.7896 \\
BiLSTM + Attention & 0.8297 & 0.8957 & 0.9692 & 0.2793 & 0.8326 & 0.7099 \\
LSTM + Attention & 0.8032 & 0.8410 & 0.7898 & 0.6727 & 0.8993 & 0.4638 \\
\bottomrule
\end{tabular}
\caption{Comprehensive model comparison. AUC=Area Under ROC Curve. Sens=Sensitivity, Spec=Specificity. Top 8 models shown.}
\label{tab:comprehensive_comparison}
\end{table*}

\FloatBarrier
\vspace{-0.2cm}

\subsection{Embedding Dimension Ablation Study}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\fontsize{8}{9}\selectfont
\begin{tabular}{@{}cccccccc@{}}
\toprule
\textbf{ID Dim} & \textbf{Static} & \textbf{Params} & \textbf{AUC} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{Acc} \\
\midrule
4 & 4 & 184K & 0.9777 & 0.9521 & 0.9441 & 0.9602 & 0.9240 \\
8 & 4 & 201K & 0.9861 & 0.9639 & 0.9532 & 0.9748 & 0.9425 \\
16 & 4 & 234K & 0.9936 & 0.9746 & 0.9665 & 0.9830 & 0.9597 \\
32 & 8 & 312K & 0.9970 & 0.9816 & 0.9869 & 0.9765 & 0.9712 \\
64 & 16 & 469K & 0.9962 & 0.9854 & 0.9870 & 0.9838 & 0.9770 \\
\bottomrule
\end{tabular}
\caption{Embedding dimension ablation. Optimal tradeoff at 32D ID + 8D static. Prec=Precision, Rec=Recall, Acc=Accuracy.}
\label{tab:embedding_ablation}
\end{table}

\vspace{-0.1cm}

\subsection{Temporal Pattern Analysis}

\begin{table}[t]
\centering
\fontsize{8}{9}\selectfont
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean obs. per subject & 6.00 ± 0.00 \\
Min obs. per subject & 6 \\
Max obs. per subject & 6 \\
Mean time gap (years) & 2.00 ± 0.00 \\
Mode time gap (years) & 2 \\
Missing rate (\%) & 11.25 \\
Regularity score & 1.000 \\
\bottomrule
\end{tabular}
\caption{Temporal pattern analysis. Perfect regularity (1.0) with 6 time points at 2-year intervals.}
\label{tab:temporal_analysis}
\end{table}

\vspace{-0.1cm}

\subsection{Component Analysis}

\subsubsection{Recurrent Unit Comparison}

\begin{table}[t]
\centering
\fontsize{9}{10}\selectfont
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{RNN} & \textbf{F1} & \textbf{Sens} & \textbf{Spec} & \textbf{Vars} \\
\midrule
BiLSTM & 0.924 & 0.970 & 0.545 & 3 \\
LSTM & 0.883 & 0.854 & 0.610 & 3 \\
GRU & 0.865 & 0.847 & 0.635 & 3 \\
GRU-D & 0.839 & 0.887 & 0.540 & 2 \\
\bottomrule
\end{tabular}
\caption{RNN unit performance (avg). BiLSTM +4-5\% F1 vs unidirectional.}
\label{tab:rnn_comparison}
\end{table}

\vspace{-0.2cm}

\textbf{Key Finding}: BiLSTM architectures consistently outperform unidirectional variants (GRU, LSTM) with average F1-score of 0.924 compared to 0.883 for LSTM and 0.865 for GRU. This 4-5\% improvement suggests that bidirectional temporal context is valuable for screening behavior prediction.

\subsubsection{Temporal Feature Encoding Strategy}

\begin{table}[t]
\centering
\fontsize{9}{10}\selectfont
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Strategy} & \textbf{F1} & \textbf{Sens} & \textbf{Models} \\
\midrule
ID + Static & 0.877 & 0.863 & 3 \\
Static Embeds & 0.867 & 0.882 & 5 \\
GRU-D & 0.843 & 0.887 & 2 \\
\bottomrule
\end{tabular}
\caption{Encoding strategy impact. ID embeddings +1.2\% F1 vs static alone.}
\label{tab:encoding_strategy}
\end{table}

\vspace{-0.2cm}

\subsubsection{Attention Mechanism Contribution}

\begin{table}[t]
\centering
\fontsize{9}{10}\selectfont
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Architecture} & \textbf{No Attn} & \textbf{+ Attn} & \textbf{Change} \\
\midrule
GRU & 0.833 & 0.939 & +0.106 \\
LSTM & 0.936 & 0.841 & -0.095 \\
BiLSTM & 0.938 & 0.896 & -0.042 \\
\bottomrule
\end{tabular}
\caption{Attention effect on F1-score. Helps GRU (+10.6\%), hurts BiLSTM (-4.2\%).}
\label{tab:attention_effect}
\end{table}

\FloatBarrier
\vspace{-0.2cm}

\textbf{Key Finding}: Attention mechanisms show non-uniform effects across RNN architectures. GRU + Attention achieves substantial improvement (+10.6\% F1-score), while BiLSTM + Attention surprisingly underperforms (-4.2\% F1-score). This suggests that bidirectional processing may already capture the temporal context benefits that attention provides. Attention adds beneficial learning flexibility for simpler architectures (GRU) but introduces optimization challenges for more complex models (BiLSTM).

\subsubsection{Interaction Effects}

We analyzed pairwise interactions between components:

\begin{enumerate}
\item \textbf{BiLSTM + ID + Static}: Best overall performance (F1=0.937, Sens=0.976)
  \begin{itemize}
  \item Bidirectional processing captures temporal patterns
  \item ID embeddings model individual-level effects
  \item Static features provide explicit temporal context
  \item Absence of attention prevents overfitting
  \end{itemize}

\item \textbf{GRU + Attention}: Strong alternative (F1=0.939, Sens=0.963)
  \begin{itemize}
  \item Attention provides learned temporal weighting
  \item GRU's reduced parameterization improves generalization
  \item Static embeddings sufficient for feature encoding
  \item Specificity (0.679) superior to BiLSTM + ID + Static (0.601)
  \end{itemize}

\item \textbf{Attention Paradox}: BiLSTM + Attention underperforms (F1=0.896)
  \begin{itemize}
  \item Possible attention mechanisms learning redundant patterns
  \item Bidirectional recurrence may make learned attention sub-optimal
  \item Increased model complexity without corresponding benefit
  \end{itemize}
\end{enumerate}

\section{Screening Behavior Trends and Descriptive Analysis}

\subsection{Mammography Screening Trends}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt}
\fontsize{7.5}{8.5}\selectfont
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Response} & \textbf{2008} & \textbf{2010} & \textbf{2012} & \textbf{2014} & \textbf{2016} & \textbf{2018} \\
\midrule
Yes (1) & 2,698 (68.1\%) & 2,671 (68.8\%) & 2,653 (70.3\%) & 2,628 (71.9\%) & 2,647 (73.9\%) & 2,626 (73.6\%) \\
No (0) & 1,266 (31.9\%) & 1,212 (31.2\%) & 1,119 (29.7\%) & 1,029 (28.1\%) & 936 (26.1\%) & 943 (26.4\%) \\
Valid Responses & 3,964 & 3,883 & 3,772 & 3,657 & 3,583 & 3,569 \\
Non-interview & 4,929 & 5,121 & 5,386 & 5,616 & 5,774 & 5,808 \\
\bottomrule
\end{tabular}
\caption{Mammography screening responses (2008-2018). Participation increased from 68.1\% to 73.6\% (+5.5 pp).}
\label{tab:mammogram_trends}
\end{table*}

\vspace{-0.15cm}

\subsection{Pap Smear Screening Trends}

\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt}
\fontsize{7.5}{8.5}\selectfont
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Response} & \textbf{2008} & \textbf{2010} & \textbf{2012} & \textbf{2014} & \textbf{2016} & \textbf{2018} \\
\midrule
Yes (1) & 2,957 (74.6\%) & 2,823 (72.8\%) & 2,588 (68.5\%) & 2,372 (64.9\%) & 2,195 (61.2\%) & 2,098 (58.9\%) \\
No (0) & 1,008 (25.4\%) & 1,057 (27.2\%) & 1,188 (31.5\%) & 1,282 (35.1\%) & 1,389 (38.8\%) & 1,463 (41.1\%) \\
Valid Responses & 3,965 & 3,880 & 3,776 & 3,654 & 3,584 & 3,561 \\
Non-interview & 4,929 & 5,121 & 5,386 & 5,616 & 5,774 & 5,808 \\
\bottomrule
\end{tabular}
\caption{Pap smear screening responses (2008-2018). Participation declined from 74.6\% to 58.9\% (-15.7 pp).}
\label{tab:pap_trends}
\end{table*}

\FloatBarrier
\vspace{-0.15cm}

\subsection{Comparative Screening Behavior Trends}

\begin{table}[t]
\centering
\fontsize{10}{11}\selectfont
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Year} & \textbf{Pap Smear} & \textbf{Mammogram} \\
\midrule
2008 & 74.6\% & 68.1\% \\
2010 & 72.8\% & 68.8\% \\
2012 & 68.5\% & 70.3\% \\
2014 & 64.9\% & 71.9\% \\
2016 & 61.2\% & 73.9\% \\
2018 & 58.9\% & 73.6\% \\
\midrule
Change & -15.7 pp & +5.5 pp \\
\bottomrule
\end{tabular}
\caption{Screening divergence: Inverse trends over 2008-2018. 21.2 pp divergence.}
\label{tab:comparative_trends}
\end{table}

\vspace{-0.15cm}

\subsection{Data Quality and Completeness Assessment}

\begin{table}[t]
\centering
\fontsize{8}{9}\selectfont
\begin{tabular}{@{}l@{}}
\toprule
\textbf{Data Quality Checklist} \\
\midrule
✓ All 119 variables: zero negative values \\
✓ Data coding: consistent across features \\
✓ Temporal alignment: verified across all points \\
✓ Feature completeness: Pass \\
\bottomrule
\end{tabular}
\caption{Quality assessment for 119 features. All variables properly coded.}
\label{tab:data_quality}
\end{table}

\FloatBarrier

\section{Discussion}

\subsection{Architecture Selection Guidelines}

Based on our ablation study, we recommend:

\paragraph{For High Sensitivity (Recall):} BiLSTM + ID + Static achieves 97.6\% sensitivity, making it suitable for screening application where false negatives are costly. The high sensitivity comes at the cost of reduced specificity (60.1\%), indicating more false positives.

\paragraph{For Balanced Performance:} GRU + Attention achieves high F1-score (0.939) with improved specificity (67.9\%), balancing sensitivity (96.3\%) and false positive rate. This configuration may be preferable when both sensitivity and specificity matter equally.

\paragraph{For Production Deployment:} BiLSTM + Static Embeds represents a middle ground (F1=0.938, Sens=0.966, Spec=0.658) with simpler architecture than ID-based models, reducing implementation and interpretability concerns.

\subsection{Computational Efficiency}

\subsection{Computational Efficiency}

\begin{table}[t]
\centering
\fontsize{9}{10}\selectfont
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Arch} & \textbf{Params} & \textbf{Time/epoch} & \textbf{F1} \\
\midrule
GRU & 25K & 12s & 0.865 \\
LSTM & 38K & 18s & 0.883 \\
BiLSTM & 38K & 22s & 0.924 \\
GRU-D & 30K & 14s & 0.843 \\
\bottomrule
\end{tabular}
\caption{Computational efficiency. BiLSTM +5.2\% F1 vs GRU justifies +83\% training time.}
\label{tab:efficiency}
\end{table}

\vspace{-0.15cm}

\subsection{Limitations}

\begin{enumerate}
\item This study focuses on a specific cancer screening dataset; generalization to other healthcare domains requires validation.
\item We fixed hidden unit dimensions; varying architecture widths may reveal different optimal configurations.
\item Attention is implemented as scaled dot-product; other attention mechanisms (multi-head, additive) were not evaluated.
\item Class imbalance was addressed through weighted loss; other techniques (SMOTE, focal loss) were not compared.
\end{enumerate}

\section{Conclusion}

This ablation study demonstrates that architectural choices substantially impact temporal neural network performance for healthcare prediction. Our key findings:

\begin{enumerate}
\item \textbf{BiLSTM superiority}: Bidirectional processing improves F1-score by 4-5\% over unidirectional variants
\item \textbf{ID embeddings matter}: Participant-level embeddings improve F1-score by 1.2\% through explicit heterogeneity modeling
\item \textbf{Attention is architecture-dependent}: Beneficial for GRU (+10.6\% F1) but detrimental for BiLSTM (-4.2\% F1)
\item \textbf{Simple is often better}: Non-attention models generally outperform attention-augmented variants in this domain
\end{enumerate}

The BiLSTM + ID + Static configuration provides optimal sensitivity (97.6\%) for high-recall screening applications, while GRU + Attention offers balanced performance. These findings provide evidence-based guidance for practitioners and highlight the importance of systematic ablation studies in architecture selection.

\section*{Acknowledgments}
We acknowledge [funding sources, data providers, institutions].

\bibliography{references}

\end{document}
