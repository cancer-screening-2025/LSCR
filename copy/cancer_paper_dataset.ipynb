{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2ded16-a40b-4ff6-8c79-4a4cebcd7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 â€” Imports & paths (RAW â†’ new_results)\n",
    "# NOTE: Configure paths once. Old data in data/raw/ stays untouched; all new outputs go to new_results/.\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile, shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype, is_integer_dtype\n",
    "\n",
    "# Repo root (your project folder)\n",
    "ROOT = Path(\"/home/adetayo/Documents/CSCI Forms/Adetayo Research/Cancer Screening Behavior/new_results/publication\")\n",
    "\n",
    "# ðŸ”¹ Inputs live in the repo ROOT (per your note)\n",
    "fresh_zip_path       = ROOT / \"data_set.zip\"  # <-- ZIP is here\n",
    "rename_template_path = ROOT / \"nlsy_variable_names_template_fresh.csv\"  # adjust if yours is elsewhere\n",
    "\n",
    "# (Optional) fallback if the template isn't in ROOT\n",
    "if not rename_template_path.exists():\n",
    "    alt = ROOT / \"data\" / \"raw\" / \"nlsy_variable_names_template_fresh.csv\"\n",
    "    if alt.exists():\n",
    "        rename_template_path = alt\n",
    "\n",
    "# ðŸ”¹ Outputs go to a fresh bucket (keeps old stuff untouched)\n",
    "RUN_ROOT   = ROOT / \"new_results\"\n",
    "INTERIM    = RUN_ROOT / \"interim\"\n",
    "PROCESSED  = RUN_ROOT / \"processed\"\n",
    "RESULTS    = RUN_ROOT / \"results\" / \"tables\"\n",
    "\n",
    "for p in [INTERIM, PROCESSED, RESULTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output file paths (all inside new_results/)\n",
    "fresh_extract_path    = INTERIM / \"Data_Reloaded\"\n",
    "renamed_csv_path      = INTERIM / \"nlsy_data_with_renamed_columns.csv\"\n",
    "females_only_path     = INTERIM / \"nlsy_data_females_only.csv\"\n",
    "negative_summary_path = RESULTS / \"remaining_negative_summary.txt\"\n",
    "final_features_path   = PROCESSED / \"final_dataset.csv\"\n",
    "\n",
    "# Keep notebook output tidy\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7998d63-baf9-40e9-89ac-64f7f4cd7a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eb48de-6ebc-4660-aed5-7f0e7450cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 â€” Helper functions\n",
    "# NOTE: Define once to keep later cells short; easy to move into src/ later.\n",
    "\n",
    "def extract_zip(zip_path: Path, out_dir: Path, clean=True):\n",
    "    \"\"\"Extract the input ZIP to out_dir. If clean=True, wipes the folder first.\"\"\"\n",
    "    if clean and out_dir.exists():\n",
    "        shutil.rmtree(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "\n",
    "def load_and_rename(csv_path: Path, rename_csv: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV and apply rename mapping from template (variable_name -> new_name).\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    rn = pd.read_csv(rename_csv)\n",
    "    rn[\"variable_name\"] = rn[\"variable_name\"].astype(str).str.strip()\n",
    "    rn[\"new_name\"] = rn[\"new_name\"].fillna(\"\").astype(str).str.strip()\n",
    "    rename_dict = dict(\n",
    "        rn.loc[rn[\"new_name\"] != \"\", [\"variable_name\",\"new_name\"]]\n",
    "          .set_index(\"variable_name\")[\"new_name\"]\n",
    "    )\n",
    "    return df.rename(columns=rename_dict)\n",
    "\n",
    "def mask_negatives(df: pd.DataFrame, cols):\n",
    "    \"\"\"Replace negatives with NaN in the provided columns.\"\"\"\n",
    "    out = df.copy()\n",
    "    if cols:\n",
    "        out.loc[:, cols] = out[cols].mask(out[cols] < 0)\n",
    "    return out\n",
    "\n",
    "def invalidate_codes(df: pd.DataFrame, cols=(\"educ\",\"mother_educ\"), invalid=(0,93,94,95)):\n",
    "    \"\"\"Set invalid codes (e.g., 0, 93â€“95) to NaN for education fields.\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].mask(out[c].isin(invalid))\n",
    "    return out\n",
    "\n",
    "def wide_to_long_years(df: pd.DataFrame, id_col=\"id_code\", years=range(2002,2023)):\n",
    "    \"\"\"Convert *_YYYY columns to long format with a 'year' column.\"\"\"\n",
    "    df = df.copy()\n",
    "    long_columns = [c for c in df.columns if any(str(y) in c for y in years)]\n",
    "    stubs = sorted({c.rsplit(\"_\",1)[0] for c in long_columns if c.rsplit(\"_\",1)[-1].isdigit()})\n",
    "    dfl = pd.wide_to_long(df, stubnames=stubs, i=id_col, j=\"year\", sep=\"_\", suffix=\"\\\\d+\").reset_index()\n",
    "    return dfl\n",
    "\n",
    "def add_time_vars(df: pd.DataFrame, allowed_years=(2008,2010,2012,2014,2016,2018), base_year=2008):\n",
    "    \"\"\"Create time_step (0..K) and time_elapsed; filter to allowed years.\"\"\"\n",
    "    out = df.copy()\n",
    "    mapping = {y:i for i,y in enumerate(sorted(allowed_years))}\n",
    "    out[\"time_step\"] = out[\"year\"].map(mapping)\n",
    "    out[\"time_elapsed\"] = out[\"year\"] - base_year\n",
    "    ymin, ymax = min(allowed_years), max(allowed_years)\n",
    "    out = out.loc[out[\"year\"].between(ymin, ymax)].sort_values([\"id_code\",\"year\"])\n",
    "    return out\n",
    "\n",
    "def add_income_lags(df, inc_col=\"inc\", group=\"id_code\", time_col=\"year\"):\n",
    "    out = df.sort_values([group, time_col]).copy()\n",
    "\n",
    "    s = pd.to_numeric(out[inc_col], errors=\"coerce\").astype(float)\n",
    "\n",
    "    # log(1+y) defined for y >= 0; negatives -> NaN\n",
    "    out[\"income_log\"] = np.where(s >= 0, np.log1p(s), np.nan)\n",
    "\n",
    "    out[\"income_log_lag1\"] = out.groupby(group)[\"income_log\"].shift(1)\n",
    "    out[\"income_log_lag2\"] = out.groupby(group)[\"income_log\"].shift(2)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_pap_lags(df: pd.DataFrame, col=\"pap_smear\", group=\"id_code\", max_lag=3):\n",
    "    \"\"\"Create pap_smear_lag1..lagK by person.\"\"\"\n",
    "    out = df.copy()\n",
    "    if col in out.columns:\n",
    "        for k in range(1, max_lag+1):\n",
    "            out[f\"{col}_lag{k}\"] = out.groupby(group)[col].shift(k)\n",
    "    return out\n",
    "\n",
    "def add_interactions(df: pd.DataFrame):\n",
    "    \"\"\"Build interactions; robust to self_assessment vs self_assement typos.\"\"\"\n",
    "    out = df.copy()\n",
    "    s50 = \"self_assessment50\" if \"self_assessment50\" in out.columns else (\"self_assement50\" if \"self_assement50\" in out.columns else None)\n",
    "    s60 = \"self_assessment60\" if \"self_assessment60\" in out.columns else (\"self_assement60\" if \"self_assement60\" in out.columns else None)\n",
    "\n",
    "    if \"income_log\" in out.columns:\n",
    "        if s50 in out.columns:\n",
    "            out[\"income_log_self50\"] = out[\"income_log\"] * out[s50]\n",
    "            out[\"income_log_lag2_self50\"] = out[\"income_log_lag2\"] * out[s50]\n",
    "        if s60 in out.columns:\n",
    "            out[\"income_log_self60\"] = out[\"income_log\"] * out[s60]\n",
    "            out[\"income_log_lag2_self60\"] = out[\"income_log_lag2\"] * out[s60]\n",
    "        for col in [\"region\",\"health_provider\",\"pap_smear_lag1\",\"health_plan\"]:\n",
    "            if col in out.columns:\n",
    "                out[f\"income_log_{col}\"] = out[\"income_log\"] * out[col]\n",
    "\n",
    "    if {\"region\",\"year\"}.issubset(out.columns):\n",
    "        out[\"region_year\"] = out[\"region\"] * out[\"year\"]\n",
    "    if {\"region\",\"health_plan\"}.issubset(out.columns):\n",
    "        out[\"region_health_plan\"] = out[\"region\"] * out[\"health_plan\"]\n",
    "    if {\"race\",\"health_plan\"}.issubset(out.columns):\n",
    "        out[\"race_health_plan\"] = out[\"race\"] * out[\"health_plan\"]\n",
    "    if {\"educ\",\"mother_educ\"}.issubset(out.columns):\n",
    "        out[\"educ_mother_educ\"] = out[\"educ\"] * out[\"mother_educ\"]\n",
    "    if {\"income_log\", \"pap_smear_lag1\"}.issubset(out.columns):\n",
    "        out['income_log_pap_smear_lag1'] = out['income_log'] * out['pap_smear_lag1']\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb526cd-483e-41f7-b1b6-3ae4c99e7eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7deb44-313b-43d0-9e8e-b14f1fd02871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H0015900</th>\n",
       "      <th>H0052200</th>\n",
       "      <th>R0000100</th>\n",
       "      <th>R0000500</th>\n",
       "      <th>R0006500</th>\n",
       "      <th>R0173600</th>\n",
       "      <th>R0214700</th>\n",
       "      <th>R0214800</th>\n",
       "      <th>R7598800</th>\n",
       "      <th>R7703700</th>\n",
       "      <th>...</th>\n",
       "      <th>T9180600</th>\n",
       "      <th>T9180700</th>\n",
       "      <th>T9184600</th>\n",
       "      <th>T9299700</th>\n",
       "      <th>T9300100</th>\n",
       "      <th>T9300200</th>\n",
       "      <th>T9300400</th>\n",
       "      <th>T9302300</th>\n",
       "      <th>T9307900</th>\n",
       "      <th>T9900000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50450</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>81000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108895</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   H0015900  H0052200  R0000100  R0000500  R0006500  R0173600  R0214700  \\\n",
       "0        -4        -4         1        58         8         5         3   \n",
       "1         3         3         2        59         5         5         3   \n",
       "2         2         3         3        61        10         5         3   \n",
       "3         4         4         4        62        11         5         3   \n",
       "4        -4        -4         5        59        12         1         3   \n",
       "\n",
       "   R0214800  R7598800  R7703700  ...  T9180600  T9180700  T9184600  T9299700  \\\n",
       "0         2        -5        -5  ...        -5        -5        -5        -5   \n",
       "1         2         1     50450  ...         1         1         1     46260   \n",
       "2         2         1        -2  ...         1         1         1     81000   \n",
       "3         2        -5        -5  ...         1         1         1    108895   \n",
       "4         1        -5        -5  ...        -5        -5        -5        -5   \n",
       "\n",
       "   T9300100  T9300200  T9300400  T9302300  T9307900  T9900000  \n",
       "0        -5        -5        -5        -5        -5        12  \n",
       "1         1         2        63         2         2        12  \n",
       "2         4         2        61         3         0        12  \n",
       "3         4         2        60         3         0        14  \n",
       "4        -5        -5        -5        -5        -5        18  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 â€” Extract from RAW â†’ find & load CSV into DataFrame\n",
    "# NOTE: Extracts into new_results/interim/Data_Reloaded/. Prefers Other_Demo.csv, otherwise selects the only CSV found.\n",
    "\n",
    "# Sanity check before extraction\n",
    "assert fresh_zip_path.exists(), f\"ZIP not found: {fresh_zip_path}\"\n",
    "\n",
    "# Extract (safe to clean; it's a NEW folder)\n",
    "extract_zip(fresh_zip_path, fresh_extract_path, clean=True)\n",
    "\n",
    "# Locate CSV\n",
    "candidates = list(fresh_extract_path.rglob(\"*.csv\"))\n",
    "target_csv = next((c for c in candidates if c.name.lower() == \"other_demo.csv\"), None)\n",
    "if target_csv is None and len(candidates) == 1:\n",
    "    target_csv = candidates[0]\n",
    "if target_csv is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find 'Other_Demo.csv' after extraction.\\n\"\n",
    "        f\"Found {len(candidates)} CSVs: {[c.name for c in candidates]}\"\n",
    "    )\n",
    "\n",
    "# Load raw\n",
    "df_raw = pd.read_csv(target_csv)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267d65f-eb2f-436d-8a9f-d3218197d229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00d5e56-343e-4e09-9b2d-463e5210823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Renamed file saved to: /home/adetayo/Documents/CSCI Forms/Adetayo Research/Cancer Screening Behavior/new_results/publication/new_results/interim/nlsy_data_with_renamed_columns.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_assement50</th>\n",
       "      <th>self_assement60</th>\n",
       "      <th>id_code</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>mother_educ</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_exam_2002</th>\n",
       "      <th>inc_2002</th>\n",
       "      <th>...</th>\n",
       "      <th>health_provider_2022</th>\n",
       "      <th>last_exam_2022</th>\n",
       "      <th>health_plan_2022</th>\n",
       "      <th>inc_2022</th>\n",
       "      <th>region_2022</th>\n",
       "      <th>marital_status_2022</th>\n",
       "      <th>age_2022</th>\n",
       "      <th>smsa_2022</th>\n",
       "      <th>hh_children_2022</th>\n",
       "      <th>educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50450</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>81000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108895</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>5</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   self_assement50  self_assement60  id_code  birth_year  mother_educ  \\\n",
       "0               -4               -4        1          58            8   \n",
       "1                3                3        2          59            5   \n",
       "2                2                3        3          61           10   \n",
       "3                4                4        4          62           11   \n",
       "4               -4               -4        5          59           12   \n",
       "\n",
       "   sample_id  race  gender  last_exam_2002  inc_2002  ...  \\\n",
       "0          5     3       2              -5        -5  ...   \n",
       "1          5     3       2               1     50450  ...   \n",
       "2          5     3       2               1        -2  ...   \n",
       "3          5     3       2              -5        -5  ...   \n",
       "4          1     3       1              -5        -5  ...   \n",
       "\n",
       "   health_provider_2022  last_exam_2022  health_plan_2022  inc_2022  \\\n",
       "0                    -5              -5                -5        -5   \n",
       "1                     1               1                 1     46260   \n",
       "2                     1               1                 1     81000   \n",
       "3                     1               1                 1    108895   \n",
       "4                    -5              -5                -5        -5   \n",
       "\n",
       "   region_2022  marital_status_2022  age_2022  smsa_2022  hh_children_2022  \\\n",
       "0           -5                   -5        -5         -5                -5   \n",
       "1            1                    2        63          2                 2   \n",
       "2            4                    2        61          3                 0   \n",
       "3            4                    2        60          3                 0   \n",
       "4           -5                   -5        -5         -5                -5   \n",
       "\n",
       "   educ  \n",
       "0    12  \n",
       "1    12  \n",
       "2    12  \n",
       "3    14  \n",
       "4    18  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 â€” Rename columns (template in RAW) â†’ save to new_results/interim\n",
    "\n",
    "df_renamed = load_and_rename(target_csv, rename_template_path)\n",
    "df_renamed.to_csv(renamed_csv_path, index=False)\n",
    "print(f\"âœ… Renamed file saved to: {renamed_csv_path}\")\n",
    "df_renamed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88298c-8226-4137-8269-28cb44ea8c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931df3ee-4722-4e86-9fbf-a3a5b5a3a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after gender==2 filter: (6283, 117)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_assement50</th>\n",
       "      <th>self_assement60</th>\n",
       "      <th>id_code</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>mother_educ</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_exam_2002</th>\n",
       "      <th>inc_2002</th>\n",
       "      <th>...</th>\n",
       "      <th>health_provider_2022</th>\n",
       "      <th>last_exam_2022</th>\n",
       "      <th>health_plan_2022</th>\n",
       "      <th>inc_2022</th>\n",
       "      <th>region_2022</th>\n",
       "      <th>marital_status_2022</th>\n",
       "      <th>age_2022</th>\n",
       "      <th>smsa_2022</th>\n",
       "      <th>hh_children_2022</th>\n",
       "      <th>educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50450</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>81000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108895</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36700</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55056</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   self_assement50  self_assement60  id_code  birth_year  mother_educ  \\\n",
       "0               -4               -4        1          58            8   \n",
       "1                3                3        2          59            5   \n",
       "2                2                3        3          61           10   \n",
       "3                4                4        4          62           11   \n",
       "7                2                4        8          58            9   \n",
       "\n",
       "   sample_id  race  gender  last_exam_2002  inc_2002  ...  \\\n",
       "0          5     3       2              -5        -5  ...   \n",
       "1          5     3       2               1     50450  ...   \n",
       "2          5     3       2               1        -2  ...   \n",
       "3          5     3       2              -5        -5  ...   \n",
       "7          6     3       2               1     36700  ...   \n",
       "\n",
       "   health_provider_2022  last_exam_2022  health_plan_2022  inc_2022  \\\n",
       "0                    -5              -5                -5        -5   \n",
       "1                     1               1                 1     46260   \n",
       "2                     1               1                 1     81000   \n",
       "3                     1               1                 1    108895   \n",
       "7                     1               1                 1     55056   \n",
       "\n",
       "   region_2022  marital_status_2022  age_2022  smsa_2022  hh_children_2022  \\\n",
       "0           -5                   -5        -5         -5                -5   \n",
       "1            1                    2        63          2                 2   \n",
       "2            4                    2        61          3                 0   \n",
       "3            4                    2        60          3                 0   \n",
       "7            1                    2        64          2                 0   \n",
       "\n",
       "   educ  \n",
       "0    12  \n",
       "1    12  \n",
       "2    12  \n",
       "3    14  \n",
       "7    14  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 â€” Filter females\n",
    "\n",
    "df_females = df_renamed.loc[df_renamed[\"gender\"] == 2].copy()\n",
    "print(\"Shape after gender==2 filter:\", df_females.shape)\n",
    "df_females.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de748de-216a-4997-8ca9-1dbeade18ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabe4b03-b14f-434d-b877-003525f5078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept rows after screening negative drop: 3087\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 â€” Drop rows with negatives in screening cols\n",
    "# NOTE: Any negative in pap_smear_* or mammogram_* drops the row (as in your original script).\n",
    "\n",
    "screening_cols = [c for c in df_females.columns if (\"pap_smear_\" in c or \"mammogram_\" in c)]\n",
    "if screening_cols:\n",
    "    df_females = df_females.loc[~df_females[screening_cols].lt(0).any(axis=1)].copy()\n",
    "print(\"Kept rows after screening negative drop:\", df_females.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f3d83-a2e1-4759-bb52-b53c7737835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec276ce-df40-4fc7-a079-ca5312eff656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-assessment cleaned (per-column float cast): ['self_assement50', 'self_assement60']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self_assement50</th>\n",
       "      <th>self_assement60</th>\n",
       "      <th>id_code</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>mother_educ</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_exam_2002</th>\n",
       "      <th>inc_2002</th>\n",
       "      <th>...</th>\n",
       "      <th>health_provider_2022</th>\n",
       "      <th>last_exam_2022</th>\n",
       "      <th>health_plan_2022</th>\n",
       "      <th>inc_2022</th>\n",
       "      <th>region_2022</th>\n",
       "      <th>marital_status_2022</th>\n",
       "      <th>age_2022</th>\n",
       "      <th>smsa_2022</th>\n",
       "      <th>hh_children_2022</th>\n",
       "      <th>educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50450</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46260</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36700</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55056</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>86400</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>107000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>175460</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>390662</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>225000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    self_assement50  self_assement60  id_code  birth_year  mother_educ  \\\n",
       "1               3.0              3.0        2          59            5   \n",
       "7               2.0              4.0        8          58            9   \n",
       "15              3.0              2.0       16          58           12   \n",
       "20              1.0              1.0       21          61           12   \n",
       "24              2.0              2.0       25          59           14   \n",
       "\n",
       "    sample_id  race  gender  last_exam_2002  inc_2002  ...  \\\n",
       "1           5     3       2               1     50450  ...   \n",
       "7           6     3       2               1     36700  ...   \n",
       "15          5     3       2               2     86400  ...   \n",
       "20          5     3       2               4    175460  ...   \n",
       "24          5     3       2               1    390662  ...   \n",
       "\n",
       "    health_provider_2022  last_exam_2022  health_plan_2022  inc_2022  \\\n",
       "1                      1               1                 1     46260   \n",
       "7                      1               1                 1     55056   \n",
       "15                     1               1                 1    107000   \n",
       "20                     1               1                 1    134000   \n",
       "24                     1               1                 1    225000   \n",
       "\n",
       "    region_2022  marital_status_2022  age_2022  smsa_2022  hh_children_2022  \\\n",
       "1             1                    2        63          2                 2   \n",
       "7             1                    2        64          2                 0   \n",
       "15            1                    3        64          2                 1   \n",
       "20            1                    2        61          2                 0   \n",
       "24            1                    2        62          2                 0   \n",
       "\n",
       "    educ  \n",
       "1     12  \n",
       "7     14  \n",
       "15    13  \n",
       "20    16  \n",
       "24    14  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 â€” Mask negatives in self-assessment cols (robust, no FutureWarning)\n",
    "# NOTE: Cast each target column to float first, then mask negatives -> NaN.\n",
    "\n",
    "\n",
    "self_cols = [\n",
    "    c for c in df_females.columns\n",
    "    if (\"self_assessment50\" in c or \"self_assessment60\" in c\n",
    "        or \"self_assement50\" in c or \"self_assement60\" in c)\n",
    "]\n",
    "\n",
    "# keep only numeric self cols\n",
    "num_self_cols = [c for c in self_cols if is_numeric_dtype(df_females[c])]\n",
    "\n",
    "changed = []\n",
    "for c in num_self_cols:\n",
    "    s = pd.to_numeric(df_females[c], errors=\"coerce\").astype(\"float64\")\n",
    "    s = s.where(s >= 0, np.nan)\n",
    "    df_females[c] = s\n",
    "    changed.append(c)\n",
    "\n",
    "print(\"Self-assessment cleaned (per-column float cast):\", changed if changed else \"none\")\n",
    "df_females.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dd459-b1fa-4cf8-a16c-eb9805de0866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a008a2-2d22-45db-ac01-bef1b9b75bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other numeric columns cleaned: {'int->Int64': 102, 'float': 0, 'total': 102}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” Mask negatives in other numeric predictors (dtype-safe, no warnings)\n",
    "# NOTE: For all other numeric columns (excluding screening/self/gender), set negatives to missing.\n",
    "#       - int columns -> cast to pandas nullable Int64 and use pd.NA\n",
    "#       - float columns -> keep float and use NaN\n",
    "\n",
    "exclude_cols = set((screening_cols or []) + (self_cols or []) + [\"gender\"])\n",
    "num_cols = [c for c in df_females.columns if is_numeric_dtype(df_females[c])]\n",
    "other_num = [c for c in num_cols if c not in exclude_cols]\n",
    "\n",
    "changed_int, changed_float = [], []\n",
    "for c in other_num:\n",
    "    s = df_females[c]\n",
    "    if is_integer_dtype(s):\n",
    "        # Cast to nullable Int64 first, then mask with pd.NA\n",
    "        s2 = s.astype(\"Int64\").mask(s < 0, pd.NA)\n",
    "        df_females[c] = s2\n",
    "        changed_int.append(c)\n",
    "    else:\n",
    "        # Ensure float dtype, then mask with NaN\n",
    "        s2 = pd.to_numeric(s, errors=\"coerce\").astype(\"float64\")\n",
    "        s2 = s2.where(s2 >= 0, np.nan)\n",
    "        df_females[c] = s2\n",
    "        changed_float.append(c)\n",
    "\n",
    "print(\"Other numeric columns cleaned:\",\n",
    "      {\"int->Int64\": len(changed_int), \"float\": len(changed_float), \"total\": len(other_num)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00812d1-5b71-4b7e-94c5-8285ce852777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbf2eec-b1bb-4995-ab8a-bdea3bfec4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>educ</th>\n",
       "      <th>mother_educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    educ  mother_educ\n",
       "1     12            5\n",
       "7     14            9\n",
       "15    13           12\n",
       "20    16           12\n",
       "24    14           14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 â€” Invalidate education/mother_educ codes\n",
    "\n",
    "# NOTE: Converts invalid codes (0, 93â€“95) to NaN.\n",
    "\n",
    "df_females = invalidate_codes(df_females, cols=(\"educ\",\"mother_educ\"), invalid=(0,93,94,95))\n",
    "df_females[[\"educ\",\"mother_educ\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754459ca-645c-48b2-b42c-918a8a8749c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6a2ba8-c20e-40cb-bea6-0a8b25c2b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final cleaned female-only dataset saved to:\n",
      "/home/adetayo/Documents/CSCI Forms/Adetayo Research/Cancer Screening Behavior/new_results/publication/new_results/interim/nlsy_data_females_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” Save cleaned females (new_results/interim)\n",
    "\n",
    "# NOTE: Persist this intermediate artifact for downstream steps.\n",
    "\n",
    "df_females.to_csv(females_only_path, index=False)\n",
    "print(f\"âœ… Final cleaned female-only dataset saved to:\\n{females_only_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ffcf5-e818-4022-a296-a31007ee7ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "215346b7-0aba-436d-b836-a388d833a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Negative value summary saved to: /home/adetayo/Documents/CSCI Forms/Adetayo Research/Cancer Screening Behavior/new_results/publication/new_results/results/tables/remaining_negative_summary.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Count_Negative_Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>self_assement50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>health_provider_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>health_provider_2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>hh_children_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>smsa_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>age_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>marital_status_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>region_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>inc_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>health_plan_2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Variable  Count_Negative_Values\n",
       "0        self_assement50                      0\n",
       "74  health_provider_2016                      0\n",
       "86  health_provider_2018                      0\n",
       "85      hh_children_2016                      0\n",
       "84             smsa_2016                      0\n",
       "83              age_2016                      0\n",
       "82   marital_status_2016                      0\n",
       "81           region_2016                      0\n",
       "80              inc_2016                      0\n",
       "79      health_plan_2016                      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 10 â€” Negative summary (new_results/results/tables)\n",
    "# NOTE: Quick sanity check that no negatives remain; writes a small TSV.\n",
    "\n",
    "neg_counts = (df_females.select_dtypes(include=[np.number]) < 0).sum()\n",
    "summary_df = neg_counts.reset_index()\n",
    "summary_df.columns = [\"Variable\", \"Count_Negative_Values\"]\n",
    "summary_df.sort_values(\"Count_Negative_Values\", ascending=False, inplace=True)\n",
    "summary_df.to_csv(negative_summary_path, index=False, sep=\"\\t\")\n",
    "print(f\"ðŸ“Š Negative value summary saved to: {negative_summary_path}\")\n",
    "summary_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794da47-9a40-46d0-9c64-27123a09eb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b251d99-45ca-43ca-8003-8d01f13d83e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>year</th>\n",
       "      <th>race</th>\n",
       "      <th>mother_educ</th>\n",
       "      <th>self_assement50</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>educ</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>self_assement60</th>\n",
       "      <th>...</th>\n",
       "      <th>health_plan</th>\n",
       "      <th>health_provider</th>\n",
       "      <th>hh_children</th>\n",
       "      <th>inc</th>\n",
       "      <th>last_exam</th>\n",
       "      <th>mammogram</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>pap_smear</th>\n",
       "      <th>region</th>\n",
       "      <th>smsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>55000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>39200</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>134636</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>170025</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>408473</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_code  year  race  mother_educ  self_assement50  sample_id  educ  \\\n",
       "0        2  2004     3            5              3.0          5    12   \n",
       "1        8  2004     3            9              2.0          6    14   \n",
       "2       16  2004     3           12              3.0          5    13   \n",
       "3       21  2004     3           12              1.0          5    16   \n",
       "4       25  2004     3           14              2.0          5    14   \n",
       "\n",
       "   birth_year  gender  self_assement60  ...  health_plan  health_provider  \\\n",
       "0          59       2              3.0  ...            1             <NA>   \n",
       "1          58       2              4.0  ...            1             <NA>   \n",
       "2          58       2              2.0  ...            1             <NA>   \n",
       "3          61       2              1.0  ...            1             <NA>   \n",
       "4          59       2              2.0  ...            1             <NA>   \n",
       "\n",
       "   hh_children     inc  last_exam  mammogram  marital_status  pap_smear  \\\n",
       "0            2   55000          1        NaN               2        NaN   \n",
       "1            1   39200          1        NaN               3        NaN   \n",
       "2            3  134636          2        NaN               2        NaN   \n",
       "3            0  170025          3        NaN               2        NaN   \n",
       "4            2  408473          1        NaN               2        NaN   \n",
       "\n",
       "   region  smsa  \n",
       "0       1     2  \n",
       "1       1     3  \n",
       "2       1     2  \n",
       "3       1     2  \n",
       "4       1     2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 11 â€” Wide â†’ long by year\n",
    "# NOTE: Converts *_YYYY columns into a tidy long format with a year column.\n",
    "\n",
    "years = range(2002, 2023)\n",
    "df_long = wide_to_long_years(df_females, id_col=\"id_code\", years=years)\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a07aa-8bdb-46b9-ba8c-464d5095cbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267db9b5-3bb6-4f38-b4e5-fc825d5c2874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>year</th>\n",
       "      <th>time_step</th>\n",
       "      <th>time_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15435</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_code  year  time_step  time_elapsed\n",
       "6174         2  2008        0.0             0\n",
       "9261         2  2010        1.0             2\n",
       "12348        2  2012        2.0             4\n",
       "15435        2  2014        3.0             6\n",
       "18522        2  2016        4.0             8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12 â€” Add time variables & restrict to 2008â€“2018\n",
    "# NOTE: Builds time_step and time_elapsed, then keeps only your target waves.\n",
    "\n",
    "df_timed = add_time_vars(df_long, allowed_years=(2008,2010,2012,2014,2016,2018), base_year=2008)\n",
    "df_timed[[\"id_code\",\"year\",\"time_step\",\"time_elapsed\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12de453-ab2e-443d-842a-ea7deaf1ae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca95c9d-7a68-4df5-b85c-4c7c61ae277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>year</th>\n",
       "      <th>inc</th>\n",
       "      <th>income_log</th>\n",
       "      <th>income_log_lag1</th>\n",
       "      <th>income_log_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>7000</td>\n",
       "      <td>8.853808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>16800</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>8.853808</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>43000</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>8.853808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15435</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>35400</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>9.729194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>10.668979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_code  year    inc  income_log  income_log_lag1  income_log_lag2\n",
       "6174         2  2008   7000    8.853808              NaN              NaN\n",
       "9261         2  2010  16800    9.729194         8.853808              NaN\n",
       "12348        2  2012  43000   10.668979         9.729194         8.853808\n",
       "15435        2  2014  35400   10.474495        10.668979         9.729194\n",
       "18522        2  2016   <NA>         NaN        10.474495        10.668979"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 13 â€” Income log and lags\n",
    "# NOTE: Creates income_log and its 1â€“2 period lags per individual.\n",
    "\n",
    "df_lagged = add_income_lags(df_timed, inc_col=\"inc\", group=\"id_code\")\n",
    "df_lagged[[\"id_code\",\"year\",\"inc\",\"income_log\",\"income_log_lag1\",\"income_log_lag2\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f8a6d-cb9a-47cf-a782-911ac387a407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c583fa-5c3b-4937-8756-80c2c6de63c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>year</th>\n",
       "      <th>pap_smear</th>\n",
       "      <th>pap_smear_lag1</th>\n",
       "      <th>pap_smear_lag2</th>\n",
       "      <th>pap_smear_lag3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15435</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_code  year  pap_smear  pap_smear_lag1  pap_smear_lag2  \\\n",
       "6174         2  2008        1.0             NaN             NaN   \n",
       "9261         2  2010        1.0             1.0             NaN   \n",
       "12348        2  2012        1.0             1.0             1.0   \n",
       "15435        2  2014        0.0             1.0             1.0   \n",
       "18522        2  2016        1.0             0.0             1.0   \n",
       "\n",
       "       pap_smear_lag3  \n",
       "6174              NaN  \n",
       "9261              NaN  \n",
       "12348             NaN  \n",
       "15435             1.0  \n",
       "18522             1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 14 â€” Pap smear lags\n",
    "# NOTE: Constructs pap_smear_lag1..lag3 per individual.\n",
    "\n",
    "df_lagged = add_pap_lags(df_lagged, col=\"pap_smear\", group=\"id_code\", max_lag=3)\n",
    "df_lagged[[\"id_code\",\"year\",\"pap_smear\",\"pap_smear_lag1\",\"pap_smear_lag2\",\"pap_smear_lag3\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e82f45-2b17-4ae3-8e95-f86b6426b24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50cfc21-bc98-482a-badb-eabd6a5566ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>year</th>\n",
       "      <th>race</th>\n",
       "      <th>mother_educ</th>\n",
       "      <th>self_assement50</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>educ</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>gender</th>\n",
       "      <th>self_assement60</th>\n",
       "      <th>...</th>\n",
       "      <th>income_log_self60</th>\n",
       "      <th>income_log_lag2_self60</th>\n",
       "      <th>income_log_region</th>\n",
       "      <th>income_log_health_provider</th>\n",
       "      <th>income_log_pap_smear_lag1</th>\n",
       "      <th>income_log_health_plan</th>\n",
       "      <th>region_year</th>\n",
       "      <th>region_health_plan</th>\n",
       "      <th>race_health_plan</th>\n",
       "      <th>educ_mother_educ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.561425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.853808</td>\n",
       "      <td>8.853808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.853808</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.187581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>9.729194</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.006936</td>\n",
       "      <td>26.561425</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>10.668979</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15435</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.423486</td>\n",
       "      <td>29.187581</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>10.474495</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.006936</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_code  year  race  mother_educ  self_assement50  sample_id  educ  \\\n",
       "6174         2  2008     3            5              3.0          5    12   \n",
       "9261         2  2010     3            5              3.0          5    12   \n",
       "12348        2  2012     3            5              3.0          5    12   \n",
       "15435        2  2014     3            5              3.0          5    12   \n",
       "18522        2  2016     3            5              3.0          5    12   \n",
       "\n",
       "       birth_year  gender  self_assement60  ...  income_log_self60  \\\n",
       "6174           59       2              3.0  ...          26.561425   \n",
       "9261           59       2              3.0  ...          29.187581   \n",
       "12348          59       2              3.0  ...          32.006936   \n",
       "15435          59       2              3.0  ...          31.423486   \n",
       "18522          59       2              3.0  ...                NaN   \n",
       "\n",
       "       income_log_lag2_self60  income_log_region  income_log_health_provider  \\\n",
       "6174                      NaN           8.853808                    8.853808   \n",
       "9261                      NaN           9.729194                    9.729194   \n",
       "12348               26.561425          10.668979                   10.668979   \n",
       "15435               29.187581          10.474495                   10.474495   \n",
       "18522               32.006936               <NA>                        <NA>   \n",
       "\n",
       "       income_log_pap_smear_lag1  income_log_health_plan  region_year  \\\n",
       "6174                         NaN                8.853808         2008   \n",
       "9261                    9.729194                9.729194         2010   \n",
       "12348                  10.668979               10.668979         2012   \n",
       "15435                  10.474495               10.474495         2014   \n",
       "18522                        NaN                    <NA>         2016   \n",
       "\n",
       "       region_health_plan  race_health_plan  educ_mother_educ  \n",
       "6174                    1                 3                60  \n",
       "9261                    1                 3                60  \n",
       "12348                   1                 3                60  \n",
       "15435                   1                 3                60  \n",
       "18522                   1                 3                60  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 15 â€” Feature interactions\n",
    "\n",
    "# NOTE: Builds interaction terms (income Ã— self assessments, region, etc.); typo-tolerant.\n",
    "\n",
    "df_feat = add_interactions(df_lagged)\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9ef5c-3dc9-4cd8-9769-4bf001b8bbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efe9e660-4043-4acf-87f1-3dd95f7ef83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final dataset with lags and interactions saved to:\n",
      " /home/adetayo/Documents/CSCI Forms/Adetayo Research/Cancer Screening Behavior/new_results/publication/new_results/processed/final_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 â€” Save final features (new_results/processed)\n",
    "# NOTE: Modeling-ready dataset saved alongside your other new artifacts.\n",
    "\n",
    "df_feat.to_csv(final_features_path, index=False)\n",
    "print(\"âœ… Final dataset with lags and interactions saved to:\\n\", final_features_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926627e-b2b1-4345-909c-990bc498b892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f778d-c759-4ccd-bc88-96f9fc749801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bc4d4-5400-4f37-aeb0-2249af81d5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b6f06a1-f3b1-465a-8698-506e50c41632",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\MamaZee\\\\EB2_NIW\\\\Publications\\\\paper_1/Preprocessing_Survey_Table/nlsy_data_with_renamed_columns.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m os.makedirs(results_figs, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# --- Load pre-negative data (contains negative survey codes) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_negative_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- Years / columns (only keep existing) ---\u001b[39;00m\n\u001b[32m     26\u001b[39m years = [\u001b[33m\"\u001b[39m\u001b[33m2008\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2010\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2012\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2014\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2016\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2018\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\MamaZee\\\\EB2_NIW\\\\Publications\\\\paper_1/Preprocessing_Survey_Table/nlsy_data_with_renamed_columns.csv'"
     ]
    }
   ],
   "source": [
    "# Screening summaries (Pap/Mammogram) from pre-negative CSV â†’ results/ (+ trend 50â€“80)\n",
    "# Build Pap/Mammogram summary tables (keeps negative codes) + trend plot (y=50â€“80)\n",
    "# No-unzip: generate screening summaries from pre-negative CSV â†’ results/\n",
    "# Screening summary & trend (Yes%): preserve negative codes, export to results/\n",
    "# Pap & Mammogram: summary tables + trend (50â€“80), using pre-negative CSV\n",
    "\n",
    "# Heading: No-unzip â€” Screening summaries from pre-negative CSV â†’ new_results/results (y=50â€“80)\n",
    "\n",
    "import os, re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths (outputs to new_results/results/*) ---\n",
    "base_path = r\"D:\\MamaZee\\EB2_NIW\\Publications\\paper_1\"\n",
    "pre_negative_csv = os.path.join(base_path, \"Preprocessing_Survey_Table\", \"nlsy_data_with_renamed_columns.csv\")\n",
    "\n",
    "results_root   = os.path.join(base_path, \"new_results\", \"results\")\n",
    "results_tables = os.path.join(results_root, \"tables\")\n",
    "results_figs   = os.path.join(results_root, \"figs\")\n",
    "os.makedirs(results_tables, exist_ok=True)\n",
    "os.makedirs(results_figs, exist_ok=True)\n",
    "\n",
    "# --- Load pre-negative data (contains negative survey codes) ---\n",
    "df = pd.read_csv(pre_negative_csv, low_memory=False)\n",
    "\n",
    "# --- Years / columns (only keep existing) ---\n",
    "years = [\"2008\", \"2010\", \"2012\", \"2014\", \"2016\", \"2018\"]\n",
    "pap_cols = [c for c in [f\"pap_smear_{y}\" for y in years] if c in df.columns]\n",
    "mam_cols = [c for c in [f\"mammogram_{y}\" for y in years] if c in df.columns]\n",
    "\n",
    "# --- Summary function (keeps negatives; % within valid Yes+No) ---\n",
    "def summarize_screening(data, cols):\n",
    "    out = {\n",
    "        \"Response\": [\n",
    "            \"Yes (1)\", \"No (0)\", \"Subtotal (Yes+No)\",\n",
    "            \"Refusal (-1)\", \"Don't know (-2)\", \"Valid skip (-4)\", \"Non-interview (-5)\",\n",
    "            \"Total (All)\"\n",
    "        ]\n",
    "    }\n",
    "    for col in cols:\n",
    "        vc   = data[col].value_counts(dropna=False)\n",
    "        c1   = int(vc.get(1, 0)); c0 = int(vc.get(0, 0))\n",
    "        cm1  = int(vc.get(-1,0)); cm2= int(vc.get(-2,0))\n",
    "        cm4  = int(vc.get(-4,0)); cm5= int(vc.get(-5,0))\n",
    "        valid = c1 + c0\n",
    "        total = c1 + c0 + cm1 + cm2 + cm4 + cm5  # exclude NA bucket\n",
    "        ypct = f\"{(c1/valid*100):.2f}%\" if valid>0 else \"0.00%\"\n",
    "        npct = f\"{(c0/valid*100):.2f}%\" if valid>0 else \"0.00%\"\n",
    "        out[col] = [f\"{c1} ({ypct})\", f\"{c0} ({npct})\", str(valid),\n",
    "                    str(cm1), str(cm2), str(cm4), str(cm5), str(total)]\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "pap_summary = summarize_screening(df, pap_cols) if pap_cols else pd.DataFrame()\n",
    "mam_summary = summarize_screening(df, mam_cols) if mam_cols else pd.DataFrame()\n",
    "\n",
    "# --- Save summaries to new_results/results/tables ---\n",
    "if not pap_summary.empty:\n",
    "    pap_path = os.path.join(results_tables, \"pap_smear_summary_with_subtotals.csv\")\n",
    "    pap_summary.to_csv(pap_path, index=False)\n",
    "    print(\"Saved:\", pap_path)\n",
    "\n",
    "if not mam_summary.empty:\n",
    "    mam_path = os.path.join(results_tables, \"mammogram_summary_with_subtotals.csv\")\n",
    "    mam_summary.to_csv(mam_path, index=False)\n",
    "    print(\"Saved:\", mam_path)\n",
    "\n",
    "# --- Optional: save subset (ID + screening columns) to tables ---\n",
    "subset_cols = ([\"id_code\"] if \"id_code\" in df.columns else []) + pap_cols + mam_cols\n",
    "if subset_cols:\n",
    "    subset_out = os.path.join(results_tables, \"pap_mammogram_2008_2018_subset.csv\")\n",
    "    df[subset_cols].to_csv(subset_out, index=False)\n",
    "    print(\"Saved:\", subset_out)\n",
    "\n",
    "# --- Build trend table (Yes% only) from summaries ---\n",
    "def extract_percent(s):\n",
    "    if isinstance(s, str) and \"(\" in s:\n",
    "        return float(s.split(\"(\")[-1].replace(\")\", \"\").replace(\"%\",\"\"))\n",
    "    return np.nan\n",
    "\n",
    "def trend_from_summary(summary_df, label_prefix):\n",
    "    if summary_df.empty: \n",
    "        return pd.DataFrame(columns=[\"Year\", f\"{label_prefix} (%)\"])\n",
    "    yes_row = summary_df.iloc[0]  # \"Yes (1)\"\n",
    "    cols = [c for c in summary_df.columns if c != \"Response\"]\n",
    "    yrs  = [re.search(r\"(\\d{4})\", c).group(1) if re.search(r\"(\\d{4})\", c) else c for c in cols]\n",
    "    vals = [extract_percent(yes_row[c]) for c in cols]\n",
    "    return pd.DataFrame({\"Year\": yrs, f\"{label_prefix} (%)\": vals})\n",
    "\n",
    "pap_trend = trend_from_summary(pap_summary, \"Pap Smear\")\n",
    "mam_trend = trend_from_summary(mam_summary, \"Mammogram\")\n",
    "\n",
    "trend_df = pd.merge(pap_trend, mam_trend, on=\"Year\", how=\"outer\").sort_values(\"Year\")\n",
    "trend_out = os.path.join(results_tables, \"screening_trend_table.csv\")\n",
    "trend_df.to_csv(trend_out, index=False)\n",
    "print(\"Saved:\", trend_out)\n",
    "\n",
    "# --- Plot (y-axis 50..80, tick every 5) to new_results/results/figs ---\n",
    "plt.figure(figsize=(8,5))\n",
    "if not pap_trend.empty:\n",
    "    plt.plot(pap_trend[\"Year\"].astype(int), pap_trend[\"Pap Smear (%)\"], marker='o', linewidth=2, label=\"Pap Smear (Yes %)\")\n",
    "if not mam_trend.empty:\n",
    "    plt.plot(mam_trend[\"Year\"].astype(int), mam_trend[\"Mammogram (%)\"], marker='s', linewidth=2, label=\"Mammogram (Yes %)\")\n",
    "\n",
    "xticks = sorted(pd.to_numeric(trend_df[\"Year\"], errors=\"coerce\").dropna().astype(int).unique().tolist())\n",
    "if xticks: plt.xticks(xticks)\n",
    "\n",
    "plt.xlabel(\"Survey Year\", fontsize=12)\n",
    "plt.ylabel(\"Screening Rate (%)\", fontsize=12)\n",
    "plt.title(\"Screening Uptake Trends Over Time\", fontsize=14)\n",
    "plt.ylim(50, 80)\n",
    "plt.yticks(list(range(50, 81, 5)))\n",
    "plt.grid(linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_out = os.path.join(results_figs, \"appendix_trends.png\")\n",
    "plt.savefig(fig_out, dpi=300, bbox_inches='tight')\n",
    "print(\"Saved:\", fig_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347730ad-a320-4944-a0f7-af3363ac5c59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393e294-eb8c-45b2-91ad-df3f4b4c0f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8832ea2-9ff8-4347-93de-fa80d35ac27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f05d07-a62c-4914-93e0-55ea6a40dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f4960-109b-492f-8896-9092f5ea961c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fdb34-40fe-4bef-88b2-eba8f1cd04d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0fc0a-f847-4dd2-90ad-8a64528d8944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec185c90-6e7b-49bd-962a-7d0f0ff2f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === XGBoost with RandomizedSearchCV (No SMOTE) â€“ Year-based Split ===\n",
    "# THIS IS IT!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# === Load and sort panel data ===\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# === Forecast target and engineered features ===\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['pap_smear'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# === Define features ===\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# === Train/Test split by year ===\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“† Forecasting for {test_year} â†’ predicting 2018\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"ðŸ“ Test observations for evaluation: {len(test_df)} rows\")\n",
    "\n",
    "# === Extract X and y ===\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# === Scale features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Define base model ===\n",
    "xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "\n",
    "# === Define hyperparameter grid for RandomizedSearchCV ===\n",
    "param_dist = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# === Randomized Search CV ===\n",
    "print(\"\\nðŸ” Running RandomizedSearchCV for XGBoost...\")\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best Parameters from Search:\")\n",
    "print(rand_search.best_params_)\n",
    "\n",
    "# === Retrain model with best parameters ===\n",
    "best_xgb = rand_search.best_estimator_\n",
    "best_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Compute approximate \"parameter count\" for XGBoost ===\n",
    "xgb_params = best_xgb.get_xgb_params()\n",
    "n_trees = xgb_params.get(\"n_estimators\", 100)\n",
    "max_depth = xgb_params.get(\"max_depth\", 3)\n",
    "splits_per_tree = (2 ** max_depth) - 1\n",
    "leaves_per_tree = (2 ** max_depth)\n",
    "total_splits = n_trees * splits_per_tree\n",
    "total_leaves = n_trees * leaves_per_tree\n",
    "approx_params = total_splits + total_leaves\n",
    "\n",
    "print(\"\\nðŸ“Š XGBoost Model Complexity:\")\n",
    "print(f\"âž¡ Number of Trees (n_estimators): {n_trees}\")\n",
    "print(f\"âž¡ Max Depth per Tree: {max_depth}\")\n",
    "print(f\"âž¡ Approx. Total Splits: {total_splits}\")\n",
    "print(f\"âž¡ Approx. Total Leaves: {total_leaves}\")\n",
    "print(f\"âž¡ ðŸ”¢ Approx. Total 'Parameters': {approx_params}\")\n",
    "\n",
    "# === Predict and Evaluate ===\n",
    "y_pred_probs = best_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# === AUC ===\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ Final ROC AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === SHAP Explainability ===\n",
    "print(\"\\nâš¡ Running SHAP Analysis...\")\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# === Rename SHAP features for interpretability ===\n",
    "feature_name_map = {\n",
    "    \"pap_smear_lag1\": \"Lagged Pap Smear\",\n",
    "    \"rolling_mean_lag1_3\": \"3-Round Screening Momentum\",\n",
    "    \"income_log_pap_smear_lag1\": \"Log Income (Lag 1)\",\n",
    "    \"race\": \"Race\",\n",
    "    \"time_step\": \"Time Step\",\n",
    "    \"health_facility\": \"Health Facility Access\",\n",
    "    \"educ_mother_educ\": \"Mother's Education\",\n",
    "    \"income_log\": \"Log Income\",\n",
    "    \"income_log_self60\": \"Income (Age 60)\",\n",
    "    \"trend_income_log\": \"Income Trend\",\n",
    "    \"region_year\": \"Region-Year Interaction\",\n",
    "    \"income_log_region\": \"Regional Income\",\n",
    "    \"health_plan\": \"Health Plan\",\n",
    "    \"year\": \"Survey Year\",\n",
    "    \"income_log_self50\": \"Income (Age 50)\",\n",
    "    \"mother_educ\": \"Motherâ€™s Education\",\n",
    "    \"educ\": \"Education\",\n",
    "    \"hh_children\": \"Children in Household\",\n",
    "    \"self_assement50\": \"Self-Health Rating (Age 50)\",\n",
    "    \"income_log_health_provider\": \"Income-Health Provider Interaction\",\n",
    "    \"marital_status\": \"Marital Status\",\n",
    "    \"region\": \"Region\",\n",
    "    \"health_provider\": \"Health Provider\",\n",
    "    \"race_health_plan\": \"Race-Health Plan Interaction\",\n",
    "    \"region_health_plan\": \"Region-Health Plan Interaction\"\n",
    "}\n",
    "\n",
    "X_test_renamed = X_test.copy()\n",
    "X_test_renamed.columns = [feature_name_map.get(col, col) for col in X_test.columns]\n",
    "\n",
    "# === SHAP Bar Plot with readable feature names ===\n",
    "shap.summary_plot(shap_values, X_test_renamed, plot_type='bar')\n",
    "\n",
    "# === SHAP Beeswarm Plot with readable feature names ===\n",
    "shap.summary_plot(shap_values, X_test_renamed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e042d8-76d4-44c3-bdfc-00bfc4c31c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4227a-bf99-4ace-9efc-012aefd0a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines extracted SHAP plots from mammogram and pap smear from xgboost \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load the fixed SHAP plots\n",
    "pap_path = r'D:/MamaZee/EB2_NIW/Publications/paper_1/new_results/results/figs/pap smear SHAP.png'\n",
    "mammo_path = r'D:/MamaZee/EB2_NIW/Publications/paper_1/new_results/results/figs/mammogram SHAP fixed.png'\n",
    "\n",
    "pap_img = mpimg.imread(pap_path)\n",
    "mammo_img = mpimg.imread(mammo_path)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 12))\n",
    "\n",
    "axs[0].imshow(pap_img)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\"Pap Smear SHAP\", fontsize=18)\n",
    "\n",
    "axs[1].imshow(mammo_img)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Mammogram SHAP\", fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.92, bottom=0.08, wspace=0.05)\n",
    "\n",
    "save_path = r'D:/MamaZee/EB2_NIW/Publications/paper_1/new_results/results/figs/shap_combined_fixed.png'\n",
    "plt.savefig(save_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Combined image saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7611c-dac1-4532-9ad8-e371ed07532c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74f55b-a65c-4cb3-9f66-bf7f7b01ea88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… LSTM for Pap Smear Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” LSTM for Pap Smear Forecasting\n",
    "# No Embeddings\n",
    "# Goal: AUC ~ 0.75 consistently\n",
    "# Final \n",
    "\n",
    "# 1 LSTM layer (64 units)\n",
    "# 1 Dropout layer (30% rate)\n",
    "# 1 Dense layer (sigmoid activation for binary classification)\n",
    "# Learning rate: 0.001\n",
    "# Dropout(0.3)\n",
    "\n",
    "# Use this one:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['pap_smear'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Locked Hyperparameters for Stability\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# === Print model summary and extract total params ===\n",
    "print(\"\\nðŸ“Š LSTM Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Capture the total number of trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_lstm, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_lstm).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a9646-361a-4256-bc72-d67fbdfe471f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56e74c-35ae-436c-82bd-33b9aebad545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… LSTM (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Final for LSTM \n",
    "\n",
    "# Embedding dim: 4\n",
    "# LSTM units: 128\n",
    "# Dropout: 0.1\n",
    "# Learning rate: 0.001\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM (1 timestep)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL LSTM model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… Locked LSTM units = 128, Dropout = 0.1\n",
    "x = LSTM(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# === Print model summary and extract total params ===\n",
    "print(\"\\nðŸ“Š LSTM + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f6ae1-5c6e-444b-89fc-8905ab7c9293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dac1b7-5ecc-480f-9371-a8429e1418f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… LSTM (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "# Final \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build LSTM model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                             # just scalar\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)       # add strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)             # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings (locked)\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”’ LSTM (locked)\n",
    "x = LSTM(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === Print model summary and count parameters ===\n",
    "print(\"\\nðŸ“Š LSTM + ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccde69-b256-4854-bc8a-46ee3f7746ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc82cd-8444-4627-8ca5-2d353d3ee4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a21f11-d4cc-4779-995b-73224d24cfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c130b-a222-4536-87ee-c784553834db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… Bi-LSTM for Pap Smear Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” LSTM for Pap Smear Forecasting\n",
    "# No Embeddings\n",
    "# AUC ~ 0.7703 consistently\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['pap_smear'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— BiLSTM Model (Locked Hyperparameters for Stability)\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_lstm, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary and Count Params (AFTER training)\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š Bi-LSTM Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_lstm).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e899a8-040f-4614-bb54-58901e764d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ff74e-eeb0-4686-bd4b-bfdb18fd95c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… BiLSTM (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Same locked params: Embedding dim = 4, LSTM units = 128, Dropout = 0.1, Learning rate = 0.001\n",
    "# Final ~ 0.91\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for BiLSTM (1 timestep)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL BiLSTM model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… BiLSTM instead of single LSTM\n",
    "x = Bidirectional(LSTM(128))(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL BiLSTM model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Params\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š BiLSTM + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897c009-a4d9-40a1-b00b-c21f40b6e40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053efc63-b30e-4790-b7bb-1e24e2d3261a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… BiLSTM (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding is weak (canâ€™t memorize individuals), AUC closer to ~0.87â€“0.89\n",
    "# Final: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout, Embedding, Input,\n",
    "    Concatenate, RepeatVector, Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for BiLSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build BiLSTM model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                            # scalar embedding\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)      # strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)            # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for timestep dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”„ Replace LSTM with BiLSTM (still 128 units)\n",
    "x = Bidirectional(LSTM(128))(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train BiLSTM Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š BiLSTM + Super-Regularized ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211d5da-72fe-4e32-aa5a-2ad698523c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350c9ea-6026-4972-9af4-7f50af2d9674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU for Pap Smear Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” GRU version\n",
    "# No Embeddings\n",
    "# AUC ~ 0.75 consistently\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['pap_smear'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for GRU (same shape as LSTM)\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU Model (Locked Hyperparameters)\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(X_train_gru.shape[1], X_train_gru.shape[2])),   # âœ… GRU instead of Bi-LSTM\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_gru, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_gru, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_gru).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b0cd4-b01f-48e0-826e-abb82b963dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce1194-de8d-4e61-b298-86a1ef11f250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd63330-db9f-4e51-bdc5-13a4ee4399b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Same settings as your LSTM version:\n",
    "# - Embedding dim: 4\n",
    "# - GRU units: 128 (same as LSTM units)\n",
    "# - Dropout: 0.1\n",
    "# - Learning rate: 0.001\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for GRU (1 timestep)\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL GRU model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… GRU instead of LSTM\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e5adf-0710-4c4c-aaa7-2b409ec31e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6f1bd-c7e1-4fb6-af64-9d545f6ab455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for GRU\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build GRU model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                             # just scalar\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)       # add strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)             # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings (locked)\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”’ GRU (instead of LSTM)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU + Super-Regularized ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a11d6-1daa-4f14-ab9f-91ec61ab8f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d50e9-8b98-40eb-9d07-1fdb93f7da6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad15a8-040f-4bbd-98e9-e822c0f5ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… GRU-D for Pap Smear Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” GRU-D version\n",
    "# No Embeddings\n",
    "# Handles Missingness Explicitly\n",
    "# auc ~ 0.75\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['pap_smear'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=[target])  # âœ… Only drop rows missing the target\n",
    "test_df = test_df.dropna(subset=[target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y and ensure float conversion\n",
    "# ===========================\n",
    "X_train = train_df[features].astype(float).values\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features].astype(float).values\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Handle Missingness for GRU-D\n",
    "# ===========================\n",
    "# 1ï¸âƒ£ Create masks (1 if present, 0 if missing)\n",
    "mask_train = np.where(np.isnan(X_train), 0, 1).astype(float)\n",
    "mask_test = np.where(np.isnan(X_test), 0, 1).astype(float)\n",
    "\n",
    "# 2ï¸âƒ£ Fill missing with zero for GRU-D (model learns decay)\n",
    "X_train_filled = np.nan_to_num(X_train, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test, nan=0)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale numeric inputs (after filling)\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled)\n",
    "X_test_scaled = scaler.transform(X_test_filled)\n",
    "\n",
    "# ðŸ” Reshape for GRU (sequence length = 1 timestep)\n",
    "X_train_seq = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_seq = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "mask_train_seq = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_seq = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU-D Model (Simplified version of GRU-D)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_seq.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_seq.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… GRU takes both numeric values and mask concatenated\n",
    "combined_input = Concatenate(axis=-1)([num_input, mask_input])\n",
    "\n",
    "x = GRU(64)(combined_input)   # ðŸ”’ Locked units\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[num_input, mask_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    [X_train_seq, mask_train_seq], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_test_seq, mask_test_seq], y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Trainable Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU-D Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_seq, mask_test_seq]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6272c6-d0c2-4e5b-9322-7f4d07b807d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… GRU-D (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Handles missingness with masks and decay terms (GRU-D style)\n",
    "# Embedding dim: 4 | GRU units: 128 | Dropout: 0.1 | LR: 0.001\n",
    "\n",
    "# AUC ~ 0.85\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten, Multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§© GRU-D style missingness handling\n",
    "# ===========================\n",
    "# âœ… 1. Create masks (1 if present, 0 if missing) â†’ convert to NumPy!\n",
    "mask_train = (~X_train.isna()).astype(float).values\n",
    "mask_test = (~X_test.isna()).astype(float).values\n",
    "\n",
    "# âœ… 2. Replace NaNs with 0 (GRU-D learns to use mask for missing info)\n",
    "X_train_filled = np.nan_to_num(X_train_scaled, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test_scaled, nan=0)\n",
    "\n",
    "# âœ… 3. Reshape for GRU input\n",
    "X_train_gru = X_train_filled.reshape((X_train_filled.shape[0], 1, X_train_filled.shape[1]))\n",
    "X_test_gru = X_test_filled.reshape((X_test_filled.shape[0], 1, X_test_filled.shape[1]))\n",
    "\n",
    "mask_train_gru = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_gru = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU-D Model (Locked Hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_gru.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… Apply mask to input (GRU-D style)\n",
    "masked_input = Multiply()([num_input, mask_input])\n",
    "\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# âœ… Combine everything\n",
    "combined_input = Concatenate()([masked_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… GRU layer (128 units, dropout = 0.1)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# âœ… Build GRU-D model\n",
    "model = Model(inputs=[num_input, mask_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train GRU-D Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, mask_train_gru, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, mask_test_gru, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Trainable Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU-D + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters for reporting in your table\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, mask_test_gru, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f41d5-ce81-4566-8523-3d04cba50a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… GRU-D (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: GRU-D handles missingness with masks; ID embedding heavily regularized\n",
    "# Expected: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise, Multiply\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['pap_smear_t_plus_1'] = df.groupby('id_code')['pap_smear'].shift(-1)\n",
    "df['pap_smear_lag1'] = df.groupby('id_code')['pap_smear'].shift(1)\n",
    "df['pap_smear_lag2'] = df.groupby('id_code')['pap_smear'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['pap_smear'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_pap_smear_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'pap_smear_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'pap_smear_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§© GRU-D style missingness handling\n",
    "# ===========================\n",
    "mask_train = (~X_train.isna()).astype(float).values\n",
    "mask_test = (~X_test.isna()).astype(float).values\n",
    "\n",
    "X_train_filled = np.nan_to_num(X_train_scaled, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test_scaled, nan=0)\n",
    "\n",
    "# Reshape for GRU (and mask)\n",
    "X_train_gru = X_train_filled.reshape((X_train_filled.shape[0], 1, X_train_filled.shape[1]))\n",
    "X_test_gru = X_test_filled.reshape((X_test_filled.shape[0], 1, X_test_filled.shape[1]))\n",
    "\n",
    "mask_train_gru = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_gru = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build GRU-D model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_gru.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… GRU-D style: apply mask to input\n",
    "masked_input = Multiply()([num_input, mask_input])\n",
    "\n",
    "# ID + other embeddings\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(input_dim=num_ids, output_dim=1,\n",
    "                     embeddings_regularizer=tf.keras.regularizers.l2(1e-2))(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)\n",
    "id_embed = Dropout(0.7)(id_embed)\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand embeddings\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# âœ… Combine numeric, mask-handled input, and embeddings\n",
    "combined_input = Concatenate()([\n",
    "    masked_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# âœ… GRU (locked params)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# âœ… Build GRU-D model\n",
    "model = Model(inputs=[num_input, mask_input, id_input, race_input, educ_input, meduc_input],\n",
    "              outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, mask_train_gru, X_train_id, X_train_race, X_train_educ, X_train_meduc],\n",
    "          y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, mask_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# âœ… Print model summary and count parameters\n",
    "model.summary()\n",
    "\n",
    "total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, mask_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5d959-81b6-48bd-b122-7e751b159e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993866f0-a41e-4f0b-ad82-526df60575d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e81186-a970-4032-953f-5e6ebf3c6e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e5d48-526f-4986-b318-58e9ef19bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mammogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1399935-ffb3-4ab8-beff-64021b2d2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === XGBoost with RandomizedSearchCV (No SMOTE) â€“ Year-based Split ===\n",
    "# THIS IS IT!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# === Load and sort panel data ===\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# === Forecast target and engineered features ===\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['mammogram'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "\n",
    "# === Define features ===\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# === Train/Test split by year ===\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“† Forecasting for {test_year} â†’ predicting 2018\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "print(f\"ðŸ“ Test observations for evaluation: {len(test_df)} rows\")\n",
    "\n",
    "# === Extract X and y ===\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# === Scale features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Define base model ===\n",
    "xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "\n",
    "# === Define hyperparameter grid for RandomizedSearchCV ===\n",
    "param_dist = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# === Randomized Search CV ===\n",
    "print(\"\\nðŸ” Running RandomizedSearchCV for XGBoost...\")\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,   # ðŸ”Ÿ TRIALS\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nâœ… Best Parameters from Search:\")\n",
    "print(rand_search.best_params_)\n",
    "\n",
    "# === Retrain model with best parameters ===\n",
    "best_xgb = rand_search.best_estimator_\n",
    "best_xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Compute approximate \"parameter count\" for XGBoost ===\n",
    "xgb_params = best_xgb.get_xgb_params()\n",
    "\n",
    "n_trees = xgb_params.get(\"n_estimators\", 100)\n",
    "max_depth = xgb_params.get(\"max_depth\", 3)\n",
    "\n",
    "if n_trees is None:\n",
    "    n_trees = 100\n",
    "if max_depth is None:\n",
    "    max_depth = 3\n",
    "\n",
    "# Each tree of depth d can have (2^d - 1) splits and (2^d) leaves\n",
    "splits_per_tree = (2 ** max_depth) - 1\n",
    "leaves_per_tree = (2 ** max_depth)\n",
    "\n",
    "total_splits = n_trees * splits_per_tree\n",
    "total_leaves = n_trees * leaves_per_tree\n",
    "approx_params = total_splits + total_leaves\n",
    "\n",
    "print(\"\\nðŸ“Š XGBoost Model Complexity:\")\n",
    "print(f\"âž¡ Number of Trees (n_estimators): {n_trees}\")\n",
    "print(f\"âž¡ Max Depth per Tree: {max_depth}\")\n",
    "print(f\"âž¡ Approx. Total Splits: {total_splits}\")\n",
    "print(f\"âž¡ Approx. Total Leaves: {total_leaves}\")\n",
    "print(f\"âž¡ ðŸ”¢ Approx. Total 'Parameters': {approx_params}\")\n",
    "\n",
    "\n",
    "# === Predict and Evaluate ===\n",
    "y_pred_probs = best_xgb.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# === AUC ===\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ Final ROC AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# === SHAP Explainability ===\n",
    "print(\"\\nâš¡ Running SHAP Analysis...\")\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# === Rename SHAP features for interpretability ===\n",
    "feature_name_map = {\n",
    "    \"mammogram_lag1\": \"Lagged Mammogram\",\n",
    "    \"rolling_mean_lag1_3\": \"3-Round Screening Momentum\",\n",
    "    \"income_log_mammogram_lag1\": \"Log Income (Lag 1)\",\n",
    "    \"race\": \"Race\",\n",
    "    \"time_step\": \"Time Step\",\n",
    "    \"health_facility\": \"Health Facility Access\",\n",
    "    \"educ_mother_educ\": \"Mother's Education\",\n",
    "    \"income_log\": \"Log Income\",\n",
    "    \"income_log_self60\": \"Income (Age 60)\",\n",
    "    \"trend_income_log\": \"Income Trend\",\n",
    "    \"region_year\": \"Region-Year Interaction\",\n",
    "    \"income_log_region\": \"Regional Income\",\n",
    "    \"health_plan\": \"Health Plan\",\n",
    "    \"year\": \"Survey Year\",\n",
    "    \"income_log_self50\": \"Income (Age 50)\",\n",
    "    \"mother_educ\": \"Motherâ€™s Education\",\n",
    "    \"educ\": \"Education\",\n",
    "    \"hh_children\": \"Children in Household\",\n",
    "    \"self_assement50\": \"Self-Health Rating (Age 50)\",\n",
    "    \"income_log_health_provider\": \"Income-Health Provider Interaction\",\n",
    "    \"marital_status\": \"Marital Status\",\n",
    "    \"region\": \"Region\",\n",
    "    \"health_provider\": \"Health Provider\",\n",
    "    \"race_health_plan\": \"Race-Health Plan Interaction\",\n",
    "    \"region_health_plan\": \"Region-Health Plan Interaction\"\n",
    "}\n",
    "\n",
    "X_test_renamed = X_test.copy()\n",
    "X_test_renamed.columns = [feature_name_map.get(col, col) for col in X_test.columns]\n",
    "\n",
    "# === SHAP Bar Plot with readable feature names ===\n",
    "shap.summary_plot(shap_values, X_test_renamed, plot_type='bar')\n",
    "\n",
    "# === SHAP Beeswarm Plot with readable feature names ===\n",
    "shap.summary_plot(shap_values, X_test_renamed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431567a4-2406-464a-91f0-9f54563154c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051155f-31b4-4940-b0e5-43a1330bddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LSTM for mammogram Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” LSTM for Pap Smear Forecasting\n",
    "# No Embeddings\n",
    "# Goal: AUC ~ 0.75 consistently\n",
    "# Final \n",
    "\n",
    "# 1 LSTM layer (64 units)\n",
    "# 1 Dropout layer (30% rate)\n",
    "# 1 Dense layer (sigmoid activation for binary classification)\n",
    "# Learning rate: 0.001\n",
    "# Dropout(0.3)\n",
    "\n",
    "# Use this one:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['mammogram'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Locked Hyperparameters for Stability\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# === Print model summary and extract total params ===\n",
    "print(\"\\nðŸ“Š LSTM Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Capture the total number of trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_lstm, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_lstm).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6aa1f5-5f60-47d6-be69-e2ca09f6f453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91284cd0-591a-4ea1-9a00-fb80d3e87a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LSTM (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Final for LSTM \n",
    "\n",
    "# Embedding dim: 4\n",
    "# LSTM units: 128\n",
    "# Dropout: 0.1\n",
    "# Learning rate: 0.001\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM (1 timestep)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL LSTM model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… Locked LSTM units = 128, Dropout = 0.1\n",
    "x = LSTM(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# === Print model summary and extract total params ===\n",
    "print(\"\\nðŸ“Š LSTM + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cbb67-3323-44c4-9b5b-6604f7fe62f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa9046-9dd1-4035-b06c-7fff4577fcc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… LSTM (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "# Final \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build LSTM model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                             # just scalar\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)       # add strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)             # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings (locked)\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”’ LSTM (locked)\n",
    "x = LSTM(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === Print model summary and count parameters ===\n",
    "print(\"\\nðŸ“Š LSTM + ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba727b7-73ae-48bf-9938-1fc6c82da76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcf13c-2a65-4201-8544-26713247218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Bi-LSTM for Pap Smear Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” LSTM for Pap Smear Forecasting\n",
    "# No Embeddings\n",
    "# AUC ~ 0.7703 consistently\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['mammogram'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for LSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— BiLSTM Model (Locked Hyperparameters for Stability)\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]))),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_lstm, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary and Count Params (AFTER training)\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š Bi-LSTM Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_lstm).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a3752-4564-4592-85ad-4ded7bebace0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4058b2a-b534-45f2-8d66-b0ed03ae8002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… BiLSTM (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Same locked params: Embedding dim = 4, LSTM units = 128, Dropout = 0.1, Learning rate = 0.001\n",
    "# Final ~ 0.91\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for BiLSTM (1 timestep)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL BiLSTM model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… BiLSTM instead of single LSTM\n",
    "x = Bidirectional(LSTM(128))(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL BiLSTM model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Params\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š BiLSTM + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04906612-7cd5-42a6-9d8d-ecd940bb08a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bfc7d-5dbf-4d36-b686-22ac1f9d314c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… BiLSTM (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding is weak (canâ€™t memorize individuals), AUC closer to ~0.87â€“0.89\n",
    "# Final: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout, Embedding, Input,\n",
    "    Concatenate, RepeatVector, Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for BiLSTM\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build BiLSTM model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_lstm.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                            # scalar embedding\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)      # strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)            # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for timestep dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”„ Replace LSTM with BiLSTM (still 128 units)\n",
    "x = Bidirectional(LSTM(128))(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train BiLSTM Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_lstm, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š BiLSTM + Super-Regularized ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_lstm, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b65e11-858f-4ab1-aa14-566758f02cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd41ed5-dc6d-46d4-b38b-48318a6b563e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU for Mammogram Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” GRU version\n",
    "# No Embeddings\n",
    "# AUC ~ 0.75 consistently\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['mammogram'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# ===========================\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "# ===========================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ðŸ” Reshape for GRU (same shape as LSTM)\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU Model (Locked Hyperparameters)\n",
    "# ===========================\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(X_train_gru.shape[1], X_train_gru.shape[2])),   # âœ… GRU instead of Bi-LSTM\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    X_train_gru, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_gru, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict(X_test_gru).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4149649-3258-4e5a-9d58-52241adef03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d885e0-c99c-4c45-9e9e-f00df1a182ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Same settings as your LSTM version:\n",
    "# - Embedding dim: 4\n",
    "# - GRU units: 128 (same as LSTM units)\n",
    "# - Dropout: 0.1\n",
    "# - Learning rate: 0.001\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "\n",
    "# Only non-time-varying features that are NOT embedded\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for GRU (1 timestep)\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build FINAL GRU model (LOCKED hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine inputs\n",
    "combined_input = Concatenate()([num_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… GRU instead of LSTM\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# âœ… Locked learning rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train FINAL model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count total trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3510c-c463-4408-90fa-611ed100e36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a7243-dadd-47af-a44a-9f427fc7d4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for GRU\n",
    "X_train_gru = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_gru = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build GRU model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(\n",
    "    input_dim=num_ids,\n",
    "    output_dim=1,                             # just scalar\n",
    "    embeddings_regularizer=tf.keras.regularizers.l2(1e-2)\n",
    ")(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)       # add strong noise\n",
    "id_embed = Dropout(0.7)(id_embed)             # very heavy dropout\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings (locked)\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "combined_input = Concatenate()([\n",
    "    num_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# ðŸ”’ GRU (instead of LSTM)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[num_input, id_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "# ðŸ”’ Locked learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, X_train_id, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU + Super-Regularized ID Embedding Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebdac0-d4e1-4550-8a02-6e65baa43c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9db12-2579-44d9-973a-102e653289ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU-D for mammogram Forecasting (No SMOTE, Locked Hyperparameters, Class Weights)\n",
    "# ðŸš« SMOTE removed â€” GRU-D version\n",
    "# No Embeddings\n",
    "# Handles Missingness Explicitly\n",
    "# auc ~ 0.75\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = df.groupby('id_code')['mammogram'].shift(1).rolling(3).mean().reset_index(level=0, drop=True).fillna(0)\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = df.groupby('id_code')['self_assement50'].expanding().mean().reset_index(level=0, drop=True)\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§® Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['race', 'educ', 'mother_educ', 'self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“… Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=[target])  # âœ… Only drop rows missing the target\n",
    "test_df = test_df.dropna(subset=[target])\n",
    "print(f\"After dropping NAs -> Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y and ensure float conversion\n",
    "# ===========================\n",
    "X_train = train_df[features].astype(float).values\n",
    "y_train = train_df[target].astype(int)\n",
    "X_test = test_df[features].astype(float).values\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# âš–ï¸ Class weights instead of SMOTE\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"\\nâš–ï¸ Class Weights: {class_weight_dict}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Handle Missingness for GRU-D\n",
    "# ===========================\n",
    "# 1ï¸âƒ£ Create masks (1 if present, 0 if missing)\n",
    "mask_train = np.where(np.isnan(X_train), 0, 1).astype(float)\n",
    "mask_test = np.where(np.isnan(X_test), 0, 1).astype(float)\n",
    "\n",
    "# 2ï¸âƒ£ Fill missing with zero for GRU-D (model learns decay)\n",
    "X_train_filled = np.nan_to_num(X_train, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test, nan=0)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale numeric inputs (after filling)\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled)\n",
    "X_test_scaled = scaler.transform(X_test_filled)\n",
    "\n",
    "# ðŸ” Reshape for GRU (sequence length = 1 timestep)\n",
    "X_train_seq = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_seq = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "mask_train_seq = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_seq = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU-D Model (Simplified version of GRU-D)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_seq.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_seq.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… GRU takes both numeric values and mask concatenated\n",
    "combined_input = Concatenate(axis=-1)([num_input, mask_input])\n",
    "\n",
    "x = GRU(64)(combined_input)   # ðŸ”’ Locked units\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[num_input, mask_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ›‘ Early stopping\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train model (No SMOTE, With Class Weights)\n",
    "# ===========================\n",
    "model.fit(\n",
    "    [X_train_seq, mask_train_seq], y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_test_seq, mask_test_seq], y_test),\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Trainable Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU-D Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_seq, mask_test_seq]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "print(\"\\nðŸ§¾ Confusion Matrix:\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78869977-c40d-496f-8ad6-849cfdc7300e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438b798-9b3a-4e08-b834-33284c428765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU-D (Locked Hyperparameters) + Embeddings for Time-Invariant Vars (race, educ, mother_educ)\n",
    "# Handles missingness with masks and decay terms (GRU-D style)\n",
    "# Embedding dim: 4 | GRU units: 128 | Dropout: 0.1 | LR: 0.001\n",
    "\n",
    "# AUC ~ 0.85\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector, Flatten, Multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding (NO id_code embedding)\n",
    "# ===========================\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "\n",
    "non_time_varying = ['self_assement50', 'self_assement60']\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "print(f\"\\nðŸ“… Forecasting for {test_year} (target = 2018)\")\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "print(f\"Before dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "print(f\"After dropping NAs â†’ Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ” Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§© GRU-D style missingness handling\n",
    "# ===========================\n",
    "# âœ… 1. Create masks (1 if present, 0 if missing) â†’ convert to NumPy!\n",
    "mask_train = (~X_train.isna()).astype(float).values\n",
    "mask_test = (~X_test.isna()).astype(float).values\n",
    "\n",
    "# âœ… 2. Replace NaNs with 0 (GRU-D learns to use mask for missing info)\n",
    "X_train_filled = np.nan_to_num(X_train_scaled, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test_scaled, nan=0)\n",
    "\n",
    "# âœ… 3. Reshape for GRU input\n",
    "X_train_gru = X_train_filled.reshape((X_train_filled.shape[0], 1, X_train_filled.shape[1]))\n",
    "X_test_gru = X_test_filled.reshape((X_test_filled.shape[0], 1, X_test_filled.shape[1]))\n",
    "\n",
    "mask_train_gru = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_gru = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— GRU-D Model (Locked Hyperparameters)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_gru.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… Apply mask to input (GRU-D style)\n",
    "masked_input = Multiply()([num_input, mask_input])\n",
    "\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# âœ… Locked embedding dim = 4\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand for time-step dimension\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# âœ… Combine everything\n",
    "combined_input = Concatenate()([masked_input, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded])\n",
    "\n",
    "# âœ… GRU layer (128 units, dropout = 0.1)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# âœ… Build GRU-D model\n",
    "model = Model(inputs=[num_input, mask_input, race_input, educ_input, meduc_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train GRU-D Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, mask_train_gru, X_train_race, X_train_educ, X_train_meduc], y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, mask_test_gru, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“Š Print Model Summary & Count Trainable Parameters\n",
    "# ===========================\n",
    "print(\"\\nðŸ“Š GRU-D + Static Embeddings Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Count trainable parameters for reporting in your table\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, mask_test_gru, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68569b78-f97b-4e4e-b0d7-e792cdba4e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50699c03-e970-43ad-80b9-daf623aa85ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# âœ… GRU-D (Locked Params) + Super-Regularized ID Embedding\n",
    "# Goal: GRU-D handles missingness with masks; ID embedding heavily regularized\n",
    "# Expected: ID embedding too weak to memorize, AUC should drop closer to 0.87â€“0.89\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU, Dense, Dropout, Embedding, Input, Concatenate, RepeatVector,\n",
    "    Flatten, GaussianNoise, Multiply\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‚ Load & Sort Panel Data\n",
    "# ===========================\n",
    "df = pd.read_csv(\"XGBoost.csv\")\n",
    "df = df.sort_values(by=[\"id_code\", \"year\"])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”¢ Prepare categorical vars for embedding\n",
    "# ===========================\n",
    "df['id_code_int'] = df['id_code'].astype('category').cat.codes\n",
    "df['race_int'] = df['race'].astype('category').cat.codes\n",
    "df['educ_int'] = df['educ'].astype('category').cat.codes\n",
    "df['mother_educ_int'] = df['mother_educ'].astype('category').cat.codes\n",
    "\n",
    "num_ids = df['id_code_int'].nunique()\n",
    "num_race = df['race_int'].nunique()\n",
    "num_educ = df['educ_int'].nunique()\n",
    "num_meduc = df['mother_educ_int'].nunique()\n",
    "\n",
    "# ===========================\n",
    "# ðŸŽ¯ Forecast target + engineered features\n",
    "# ===========================\n",
    "df['mammogram_t_plus_1'] = df.groupby('id_code')['mammogram'].shift(-1)\n",
    "df['mammogram_lag1'] = df.groupby('id_code')['mammogram'].shift(1)\n",
    "df['mammogram_lag2'] = df.groupby('id_code')['mammogram'].shift(2)\n",
    "df['rolling_mean_lag1_3'] = (\n",
    "    df.groupby('id_code')['mammogram'].shift(1)\n",
    "      .rolling(3).mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    "      .fillna(0)\n",
    ")\n",
    "df['trend_income_log'] = df.groupby('id_code')['income_log'].diff().fillna(0)\n",
    "df['cumulative_avg_self_assess50'] = (\n",
    "    df.groupby('id_code')['self_assement50']\n",
    "      .expanding().mean()\n",
    "      .reset_index(level=0, drop=True)\n",
    ")\n",
    "df['first_year'] = df.groupby('id_code').cumcount() == 0\n",
    "\n",
    "# interraction:\n",
    "df['income_log_mammogram_lag1'] = df.groupby('id_code')['income_log'].shift(1) * df.groupby('id_code')['mammogram'].shift(1)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“‹ Features\n",
    "# ===========================\n",
    "time_varying = [\n",
    "    'marital_status', 'time_step', 'health_plan', 'region', 'year',\n",
    "    'hh_children', 'health_facility', 'health_provider',\n",
    "    'income_log_self50', 'income_log_self60',\n",
    "    'income_log', 'region_year', 'region_health_plan',\n",
    "    'race_health_plan', 'income_log_region', 'income_log_health_provider',\n",
    "    'income_log_mammogram_lag1', 'educ_mother_educ', 'trend_income_log',\n",
    "    'rolling_mean_lag1_3', 'mammogram_lag1'\n",
    "]\n",
    "non_time_varying = ['self_assement50', 'self_assement60']  # race/educ/mother_educ embedded separately\n",
    "\n",
    "features = time_varying + non_time_varying\n",
    "target = 'mammogram_t_plus_1'\n",
    "\n",
    "# ===========================\n",
    "# ðŸ—“ Forecasting for 2016 â†’ predict 2018\n",
    "# ===========================\n",
    "train_years = [2008, 2010, 2012, 2014, 2016]\n",
    "test_year = 2016\n",
    "\n",
    "train_df = df[df['year'].isin(train_years)]\n",
    "test_df = df[df['year'] == test_year]\n",
    "\n",
    "train_df = train_df.dropna(subset=features + [target])\n",
    "test_df = test_df.dropna(subset=features + [target])\n",
    "\n",
    "# ===========================\n",
    "# ðŸ”„ Split X/y\n",
    "# ===========================\n",
    "X_train = train_df[features]\n",
    "X_test = test_df[features]\n",
    "y_train = train_df[target].astype(int)\n",
    "y_test = test_df[target].astype(int)\n",
    "\n",
    "# Extract categorical vars for embedding\n",
    "X_train_id = train_df['id_code_int'].values\n",
    "X_test_id = test_df['id_code_int'].values\n",
    "X_train_race = train_df['race_int'].values\n",
    "X_test_race = test_df['race_int'].values\n",
    "X_train_educ = train_df['educ_int'].values\n",
    "X_test_educ = test_df['educ_int'].values\n",
    "X_train_meduc = train_df['mother_educ_int'].values\n",
    "X_test_meduc = test_df['mother_educ_int'].values\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ Scale numeric features\n",
    "# ===========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===========================\n",
    "# ðŸ§© GRU-D style missingness handling\n",
    "# ===========================\n",
    "mask_train = (~X_train.isna()).astype(float).values\n",
    "mask_test = (~X_test.isna()).astype(float).values\n",
    "\n",
    "X_train_filled = np.nan_to_num(X_train_scaled, nan=0)\n",
    "X_test_filled = np.nan_to_num(X_test_scaled, nan=0)\n",
    "\n",
    "# Reshape for GRU (and mask)\n",
    "X_train_gru = X_train_filled.reshape((X_train_filled.shape[0], 1, X_train_filled.shape[1]))\n",
    "X_test_gru = X_test_filled.reshape((X_test_filled.shape[0], 1, X_test_filled.shape[1]))\n",
    "\n",
    "mask_train_gru = mask_train.reshape((mask_train.shape[0], 1, mask_train.shape[1]))\n",
    "mask_test_gru = mask_test.reshape((mask_test.shape[0], 1, mask_test.shape[1]))\n",
    "\n",
    "# ===========================\n",
    "# ðŸ— Build GRU-D model (Locked Params + Weak ID Embedding)\n",
    "# ===========================\n",
    "num_input = Input(shape=(1, X_train_gru.shape[2]), name='num_input')\n",
    "mask_input = Input(shape=(1, X_train_gru.shape[2]), name='mask_input')\n",
    "\n",
    "# âœ… GRU-D style: apply mask to input\n",
    "masked_input = Multiply()([num_input, mask_input])\n",
    "\n",
    "# ID + other embeddings\n",
    "id_input = Input(shape=(1,), name='id_input')\n",
    "race_input = Input(shape=(1,), name='race_input')\n",
    "educ_input = Input(shape=(1,), name='educ_input')\n",
    "meduc_input = Input(shape=(1,), name='meduc_input')\n",
    "\n",
    "# ðŸ”’ ID embedding (super regularized)\n",
    "id_embed = Embedding(input_dim=num_ids, output_dim=1,\n",
    "                     embeddings_regularizer=tf.keras.regularizers.l2(1e-2))(id_input)\n",
    "id_embed = GaussianNoise(0.3)(id_embed)\n",
    "id_embed = Dropout(0.7)(id_embed)\n",
    "id_embed_flat = Flatten()(id_embed)\n",
    "id_embed_expanded = RepeatVector(1)(id_embed_flat)\n",
    "\n",
    "# ðŸ”’ Time-invariant embeddings\n",
    "race_embed = Embedding(input_dim=num_race, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(race_input)\n",
    "educ_embed = Embedding(input_dim=num_educ, output_dim=4,\n",
    "                       embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(educ_input)\n",
    "meduc_embed = Embedding(input_dim=num_meduc, output_dim=4,\n",
    "                        embeddings_regularizer=tf.keras.regularizers.l2(1e-4))(meduc_input)\n",
    "\n",
    "# Flatten & expand embeddings\n",
    "race_embed_expanded = RepeatVector(1)(Flatten()(race_embed))\n",
    "educ_embed_expanded = RepeatVector(1)(Flatten()(educ_embed))\n",
    "meduc_embed_expanded = RepeatVector(1)(Flatten()(meduc_embed))\n",
    "\n",
    "# âœ… Combine numeric, mask-handled input, and embeddings\n",
    "combined_input = Concatenate()([\n",
    "    masked_input, id_embed_expanded, race_embed_expanded, educ_embed_expanded, meduc_embed_expanded\n",
    "])\n",
    "\n",
    "# âœ… GRU (locked params)\n",
    "x = GRU(128)(combined_input)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# âœ… Build GRU-D model\n",
    "model = Model(inputs=[num_input, mask_input, id_input, race_input, educ_input, meduc_input],\n",
    "              outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ===========================\n",
    "# ðŸš€ Train Model\n",
    "# ===========================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit([X_train_gru, mask_train_gru, X_train_id, X_train_race, X_train_educ, X_train_meduc],\n",
    "          y_train,\n",
    "          epochs=60,\n",
    "          batch_size=32,\n",
    "          validation_data=([X_test_gru, mask_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc], y_test),\n",
    "          callbacks=[early_stop],\n",
    "          verbose=1)\n",
    "\n",
    "# âœ… Print model summary and count parameters\n",
    "model.summary()\n",
    "\n",
    "total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f\"\\nðŸ”¢ Total Trainable Parameters: {total_params}\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ðŸ“ˆ Evaluate\n",
    "# ===========================\n",
    "y_pred_probs = model.predict([X_test_gru, mask_test_gru, X_test_id, X_test_race, X_test_educ, X_test_meduc]).ravel()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nâœ… Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix ({test_year} â†’ 2018)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f\"\\nðŸ”µ ROC AUC Score: {auc_score:.4f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve (2016 â†’ 2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c58b6d-0a18-4c4a-ac29-da20b8460d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee9d67-0610-4be7-b21c-d6144dfe0a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
